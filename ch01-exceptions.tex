\chapter{Exceptions in programming languages}

``Exceptions'' is a broader term referring to a strategy of handling erroneous
states in computer programs by interrupting normal program execution, running special
code called an \emph{exception handler} and then resuming execution in a known, different
state.

The usual terminology is not very strict: the word ``exception'' may mean sligtly
different things -- or different sides of the same thing -- in different contexts.
For example, programming languages are said to \emph{have exceptions} if they support
this kind of error handling; exceptions are said to \emph{be compiled}, while it is the
corresponding infrastructure and support code that is compiled; often the word ``exception''
denotes a piece of information about the error being handled; et cetera.

When an error occurs during normal execution of a program, \emph{an exception is thrown}%
\footnote{Some programming languages, such as Python, use the term \emph{raised}.%
\cite{python:reference}}.
This starts the
process of \emph{handling the exception}: looking for a suitable
\emph{exception handler} that \emph{handles the exception}, either by \emph{catching} it
to resume normal computation, or by \emph{re-throwing} it to find another handler
able to deal with the error.

Due to common names of the corresponding syntactic features of popular programming languages%
\footnote{Most of them use the same keywords for this purpose.},
a piece of code together with attached pieces of handler code is called a \emph{try-block},
and a piece of handler code is called a \emph{catch-block}.

Most languages also provide \emph{finally-blocks}. These are pieces of code attached to a
try-block that are guaranteed to be executed after the try-block, whether an exception
has been thrown or not. Because of this property, finally-blocks are usually used to clean up
resources. In this thesis, we will not model finally-blocks as these can be
supplemented by an appropriate use of all-catching exception handlers.

There may be multiple handlers attached to a piece of code.
As already mentioned, the word ``exception'' also denotes a piece of information about
the error or condition causing the exceptional state, modeled by a value of the programming
language. In typed languages, these values have types, exception handlers declare what
types of exceptions they can handle, and based on the type of the thrown exception,
an appropriate handler is selected. The handler can then inspect the exception and
behave appropriately.

A try-block needn't have handlers for all exceptions that might arise within. If an exception
is \emph{uncaught} within a try-block, it is \emph{propagated} to the containing try-block,
which may not catch this exception as well, propagating it further.
If an exception propagates all the way out of all nested try-blocks, the program usually
aborts.

To give a quick illustration how try-blocks look in the concrete syntax of some widely used
languages, \Fref{fig:try-blocks} contains four examples.\footnote{Note that OCaml does not
have syntax for finally-blocks; these are simulated by a function. Haskell does not have
syntax for exceptions at all, both \emph{catch} and \emph{finally} are just functions.
All code snippets are just symbolic and have been stripped of non-relevant context, such
as library imports and the definitions of the functions \emph{perform\_work}
and \emph{do\_cleanup}.}

\begin{figure}
%
\begin{subfigure}[b]{0.46\textwidth}\begin{codepy}
try:
	perform_work()
except IOError:
	print "IO error caught"
finally:
	do_cleanup()
\end{codepy}\caption{Python}\end{subfigure}
%
\begin{subfigure}[b]{0.46\textwidth}\begin{codejava}
try {
	performWork();
} catch (IOException e) {
	System.out.println(
	    "IO error caught");
} finally {
	doCleanup();
}
\end{codejava}\caption{Java}\end{subfigure}

\begin{subfigure}[b]{0.46\textwidth}\begin{codehs}
performWork
 `catch` (\(e :: IOException) ->
   putStrLn "IO error caught")
 `finally` 
   doCleanup
\end{codehs}\caption{Haskell}\end{subfigure}
%
\begin{subfigure}[b]{0.46\textwidth}\begin{codeml}
finally do_cleanup (fun () ->
  try perform_work ()
  with IO_error ->
    print_string "IO error caught"
) ()
\end{codeml}\caption{OCaml}\end{subfigure}

\caption{Try-blocks in different languages}
\label{fig:try-blocks}
\end{figure}

\todo{Really include ``finally''? Revise OCaml.}

\section{Purpose}

As already mentioned, in practice, exceptions are mostly used to handle errors or
other exceptional states. The advantage to using
exceptions for this purpose is separation of concerns and hence cleaner resulting code.
Especially when reading a program, the reader first reads the code related the expected
execution path, uncluttered with error checks, which brings forward the main idea
of the code.

However, some languages, for example Python or OCaml, use exceptions also for control-flow
purposes, not only in rare critical events. Python iterators raise an exception to
indicate the end of stream \cite{python:reference}; also file-I/O functions in OCaml raise
an exception to indicate the end of file \cite{ocaml:reference}. The implementation of
exceptions in these languages is efficient enough to make them cheap and enable this approach.

\todo{Disadvantages. Multiple exit points, reasoning.}

\todo{Extend this section.}

\section{History}

Louden and Lambert describe the invention of exceptions in their book
\emph{Programming Languages: Principles and Practice} as follows.

\begin{quote}
Exception handling was pioneered by the language PL/I in the 1960s and
significantly advanced by CLU in the 1970s, with the major design questions
eventually resolved in the 1980s and early 1990s. Today, virtually all major
current languages, including C++, Java, Ada, Python, ML, and Common Lisp (but
not C, Scheme or Smalltalk) have built-in exception handling mechanisms.
Exception handling has, in particular, been integrated very well into
object-oriented mechanisms in Python, Java, and C++, and into functional
mechanisms in ML and Common Lisp. Also, languages that do not have built-in
mechanisms sometimes have libraries available that provide them, or have other
built-in ways of simulating them. \cite[p.~423]{louden:languages}
\end{quote}

% http://stackoverflow.com/questions/1449951/what-language-was-the-first-to-implement-exception-handling

\todo{Extend this section.}

\section{Theoretical appeal}

The theoretical appeal of exceptions in function languages is impossible to discuss
without having introduced the \emph{Curry-Howard correspondence}.

\subsection{The Curry-Howard correspondence}

The Curry-Howard correspondence establishes a relationship between typed programs and logic.
According to this interpretation, types of programs correspond to logical propositions --
and programs themselves correspond to proofs of logical propositions corresponding to their
types.

This correspondence goes all the way to relating whole $\lambda$-calculi to different
logical systems. For example, the simply typed $\lambda$-calculus corresponds to the minimal
logic, see \Fref{fig:stlc-ml}.

\begin{figure}
\centering
\begin{subfigure}[b]{0.45\textwidth}
\begin{prooftree}
\bax{$x : \sigma \in \Gamma$}
\bright{Ax}\bun{$\Gamma \vdash x : \sigma$}
\end{prooftree}
\begin{prooftree}
\bax{$\Gamma, x : \sigma \vdash M : \tau$}
\bright{Abstr}\bun{$\Gamma \vdash (\lambda x:\sigma. M) : \sigma \to \tau$}
\end{prooftree}
\begin{prooftree}
\bax{$\Gamma \vdash f : \sigma \to \tau$}
\bax{$\Gamma \vdash x : \sigma$}
\bright{App}\bbin{$\Gamma \vdash f x : \tau$}
\end{prooftree}

\caption{$\lambda_\to$}
\end{subfigure}
%
\begin{subfigure}[b]{0.45\textwidth}
\begin{prooftree}
\bax{$A \in \Gamma$}
\bright{Ax}\bun{$\Gamma \vdash A$}
\end{prooftree}
\begin{prooftree}
\bax{$\Gamma, A \vdash B$}
\bright{$I_\to$}\bun{$\Gamma \vdash A \to B$}
\end{prooftree}
\begin{prooftree}
\bax{$\Gamma \vdash A \to B$}
\bax{$\Gamma \vdash A$}
\bright{$E_\to$}\bbin{$\Gamma \vdash B$}
\end{prooftree}
\caption{$ML$}
\end{subfigure}

\caption{Simply typed $\lambda$-calculus ($\lambda_\to$), compared to minimal logic (ML).}
\label{fig:stlc-ml}
\end{figure}

When put side-by-side, it can be seen that the typing rules for the simply typed
$\lambda$-calculus become precisely the natural-deduction rules for minimal logic
if terms are erased and only types are retained.

In this interpretation, \emph{function application} on the computational side corresponds
to \emph{modus ponens} (or implication elimination) on the logic side, and
\emph{$\lambda$-abstraction} corresponds to the \emph{deduction theorem} (or implication
introduction).

The correspondence between certain $\lambda$-calculi and logic systems was presented
by Barendregt in \cite{barendregt91} in the elegant form of the $\lambda$-cube, as seen in
\Fref{fig:lambda-cube}. The names of the $\lambda$-calculi and the corresponding
logic systems are listed in \Fref{tab:lambda-cube}.

\begin{figure}
\centering
\begin{subfigure}{0.4\textwidth}
\begin{tikzpicture}[x=9mm,y=9mm,
        back line/.style={densely dotted,-},
        normal line/.style={-stealth,-},
        cross line/.style={normal line,-,
           preaction={draw=white, -, 
           line width=6pt}},
    ]
    \draw
    	(0,0) node{$\lambda_\to$}
    	(3,0) node{$\lambda{}P$}
    	(0,3) node{$\lambda{}2$}
    	(3,3) node{$\lambda{}P2$}
    	(1,1) node{$\lambda\underline{\omega}$}
    	(4,1) node{$\lambda{}P\underline{\omega}$}
    	(1,4) node{$\lambda\omega$}
    	(4,4) node{$\lambda{}C$}
	    ;
	\draw
		(1,1)
		+(0.5,0) edge[back line] +(2.5,0)
		+(0,0.5) edge[back line] +(0,2.5)
		+(0.5,3) edge[normal line] +(2.5,3)
		+(3,0.5) edge[normal line] +(3,2.5)
		;
	\draw[normal line]
		(0,0)
		+(0.5,0) edge +(2.5,0)
		+(0,0.5) edge +(0,2.5)
		+(0.5,3) edge[cross line] +(2.5,3)
		+(3,0.5) edge[cross line] +(3,2.5)
		;
	\draw
		(0,3) +(0.25,0.25) edge[normal line] +(0.7,0.7)
		(3,3) +(0.25,0.25) edge[normal line] +(0.7,0.7)
		(3,0) +(0.25,0.25) edge[normal line] +(0.7,0.7)
		(0,0) +(0.25,0.25) edge[back line] +(0.7,0.7)
		;
\end{tikzpicture}
\caption{$\lambda$-cube}
\end{subfigure}
%
\begin{subfigure}{0.4\textwidth}
\begin{tikzpicture}[x=9mm,y=9mm,
        back line/.style={densely dotted,-},
        normal line/.style={-stealth,-},
        cross line/.style={normal line,-,
           preaction={draw=white, -, 
           line width=6pt}},
    ]
    \draw[font=\footnotesize]
    	(0,0) node{PROP}
    	(3,0) node{PRED}
    	(0,3) node{PROP2}
    	(3,3) node{PRED2}
    	(1,1) node{PROP$\underline{\omega}$}
    	(4,1) node{PRED$\underline{\omega}$}
    	(1,4) node{PROP$\omega$}
    	(4,4) node{PRED$\omega$}
	    ;
	\draw
		(1,1)
		+(0.8,0) edge[back line] +(2.2,0)
		+(0,0.3) edge[back line] +(0,2.7)
		+(0.8,3) edge[normal line] +(2.2,3)
		+(3,0.3) edge[normal line] +(3,2.7)
		;
	\draw[normal line]
		(0,0)
		+(0.8,0  ) edge +(2.2,0  )
		+(0  ,0.3) edge +(0  ,2.7)
		+(0.8,3  ) edge[cross line] +(2.2,3  )
		+(3  ,0.3) edge[cross line] +(3  ,2.7)
		;
	\draw
		(0,3) +(0.25,0.25) edge[normal line] +(0.7,0.7)
		(3,3) +(0.25,0.25) edge[normal line] +(0.7,0.7)
		(3,0) +(0.25,0.25) edge[normal line] +(0.7,0.7)
		(0,0) +(0.25,0.25) edge[back line] +(0.7,0.7)
		;
\end{tikzpicture}
\caption{Logic-cube}
\end{subfigure}
%
\caption{The $\lambda$-cube and the corresponding logic-cube \cite{barendregt91}}
\label{fig:lambda-cube}
\end{figure}

\begin{table}
\centering
\begin{tabular}{lll}
\toprule
\textit{$\lambda$-calculus} & \textit{logic} & \textit{description} \\
\midrule
$\lambda_\to$ & PROP & first-order propositional \\
$\lambda{}P$ & PRED & first-order predicate \\
$\lambda{}2$ & PROP2 & second-order propositional \\
$\lambda{}P2$ & PRED2 & second-order predicate \\
$\lambda\underline{\omega}$ & PROP$\underline{\omega}$ & weakly higher-order propositional \\
$\lambda{}P\underline{\omega}$& PRED$\underline{\omega}$ & weakly higher-order predicate \\
$\lambda\omega$ & PROP$\omega$ & higher-order propositional \\
$\lambda{}C$ & PRED$\omega$ & higher-order predicate \\
\bottomrule
\end{tabular}

\vspace{6pt}
{\small The calculus $\lambda_\to$ is also called the \emph{simply typed $\lambda$-calculus} (STLC)
and the calculus $\lambda{}C$ is also called the \emph{calculus of constructions} (CoC).}
\caption{The $\lambda$-calculi and logic systems of the $\lambda$-cube \cite{barendregt91}}
\label{tab:lambda-cube}
\end{table}

\todo{Talk about dependent types, Coq, proving, Type theory, \ldots ?}

\subsection{Control operators}

In functional languages, exceptions are closely related to the theory of
\emph{control operators}\footnote{Such as \ident{call/cc} in Scheme.},
pioneered by Landin \cite{landin65, thielecke98},
for example the calculus $\lC$ introduced by Felleisen in \cite{felleisen87}
as a way to reason about abortive programs. An accessible formal description
thereof can be found in \cite[p.~876, Section~3]{ariola-herbelin} or
\cite{griffin90}; here we will restrict ourselves to an informal sketch.

Felleisen introduced three control operators, $\bigC$ for ``control'', $\bigA$
for ``abort'', and $\bigK$ for \ident{call/cc}. The operator $\bigA$
takes a value and replaces the current context with it; the operator $\bigK$
takes a function and applies it to the current context (reified as a continuation),
and finally the operator $\bigC$ takes a function and replaces the current context
with the function applied to the original context, thus being a combination of
the former two; an overview can be seen in \Fref{tab:control-operators}.

\begin{table}
\centering
\begin{tabular}{llll}
\toprule
\thead{Operator} & \thead{Replaces CC*} & \thead{Provides CC**} & \thead{In terms of the others} \\
\midrule
$\bigC$ & yes & yes & $\lambda M.\,\bigK(\lambda k.\,\bigA (M k))$\\
$\bigK$ & no***	& yes & $\lambda M.\,\bigC(\lambda k.\,k (M k)) $ \\
$\bigA$ & yes & no & $\lambda M.\,\bigC(\lambda \_.\,M)$ \\
\bottomrule
\end{tabular}

\vspace{4pt}
{\footnotesize *Current context. **Applies its argument to the current continuation.
***Context replacement happens only when (if ever) the provided continuation is applied.}
\caption{Overview of control operators}
\label{tab:control-operators}
\end{table}

The behavior of these operators is best shown on an example. The simplest one is the abort
operator $\bigA$. Let $E$ be some $\lambda$-context\footnote{A formal treatise of what
a $\lambda$-context is can be found in \cite{griffin90}, here we rely on intuition.}.
Then the operator $\bigA$ reduces as follows.
\begin{equation*}
	E[\bigA(M)] \red M
\end{equation*}
For example:
\begin{equation*}
	1 + (2 \cdot \bigA(3)) \red 3
\end{equation*}
No matter what the context is\footnote{The careful reader may wonder what if $E[x] = \bigA(x)$.
Such contexts are not valid by the nature of their definition.}, all of it is replaced by the
argument to $\bigA$. Hence,
the operator $\bigA$ provides ``escape'' to the top-level. Note that any computation commenced
(here, adding the $1$ and multiplying by $2$) is discarded and evaluation starts anew
with whatever $\bigA$ took as the argument.

\begin{figure}
\centering
\begin{subfigure}[t]{0.4\textwidth}
\begin{eqnarray*}
&&2 \cdot (3 + \underline{\bigC(\lambda k.\, 4 + k\, 5)}) \\
&&{} \red 2 \cdot (3 + \underline{5}) \\
&&{} \red 16
\end{eqnarray*}
\caption{Throwing to $\bigC$}\label{fig:CK-throwC}
\end{subfigure}
%
\begin{subfigure}[t]{0.4\textwidth}
\begin{eqnarray*}
&& 2 \cdot (3 + \underline{\bigK(\lambda k.\, 4 + k\, 5)}) \\
&& {} \red 2 \cdot (3 + \underline{5}) \\
&& {} \red 16
\end{eqnarray*}
\caption{Throwing to $\bigK$}\label{fig:CK-throwK}
\end{subfigure}

\begin{subfigure}[t]{0.4\textwidth}
\begin{eqnarray*}
&& 2 \cdot (3 + \bigC(\lambda k.\, \underline{4 + 5})) \\
&& {} \red \underline{4 + 5} \\
&& {} \red 9
\end{eqnarray*}
\caption{Not throwing to $\bigC$}\label{fig:CK-nothrowC}
\end{subfigure}
%
\begin{subfigure}[t]{0.4\textwidth}
\begin{eqnarray*}
&& 2 \cdot (3 + \bigK(\lambda k.\, \underline{4 + 5})) \\
&& {} \red 2 \cdot (3 + \underline{9}) \\
&& {} \red 24
\end{eqnarray*}
\caption{Not throwing to $\bigK$}\label{fig:CK-nothrowK}
\end{subfigure}

\caption{Examples of reduction with $\bigC$ and $\bigK$}
\label{fig:CK}
\end{figure}

Now let us describe the operators $\bigC$ and $\bigK$ in \Fref{fig:CK}. Both of
them are similar in that the function passed
as the argument to either of the two operators takes one argument $k$, which represents
the context at the point of application of the control operator. In the case described
in Figure \ref{fig:CK}, the context
looks like $2 \cdot (3 + \square)$, the $\square$ being a hole placed exactly where
the control operator was located.

The argument $k$ is a continuation and it can be applied like a function -- except that it
is \emph{not} a function since applying it to an argument $x$ replaces the whole evaluation
context by the context represented by $k$, having the hole filled with the value $x$.
This happens in Figure \ref{fig:CK-throwC} and \Fref{fig:CK-throwK}.

In other words, applying the continuation $k$ to a value makes seemingly the control operator
``return'' that value immediately, much like the \ident{return} statement returns from functions
in procedural languages.

Application of the continuation $k$ to an argument is usually described as
\emph{throwing} to the continuation.

The difference between $\bigC$ and $\bigK$ shows when the function being their argument
\emph{does not throw} to the continuation and instead returns a value normally. In that case,
the operator $\bigC$ \emph{aborts} and replaces the whole evaluation context with the returned
value -- unlike the operator $\bigK$, that simply returns the value without manipulating
the context.

This can be seen in Figure \ref{fig:CK-nothrowC} and \Fref{fig:CK-nothrowK}, where the
computation $4 + 5$ either replaces the whole context ($\bigC$) or is simply returned
($\bigK$).\footnote{Apparently, here we use the call-by-name evaluation strategy, which
is arguably more suitable for demonstrational purposes. The $\lC$ calculus however comes
also in the call-by-value flavor, which is in fact the variant that fits Scheme.}

\subsubsection{Control operators and Curry-Howard}

Traditionally, logic connected with computation has naturally been intuitionistic.
However, control operators enable us to extend the Curry-Howard correspondence
further.

In 1990, Griffin showed \cite{griffin90} that the control operator $\mathcal{C}$
used in $\lC$ can be given the type $\neg \neg A \to A$, yielding a computational
interpretation of classical logic.

Going all the way to classical logic is not necessary, either. The operator $\bigA$
corresponds to the Ex-falso-quodlibet rule (EFQ), the \ident{call/cc} operator $\bigK$
corresponds to Peirce's law (PL), and, for completeness, the operator $\bigC$
corresponds to double negation elimination (DN) \cite[p.~877]{ariola-herbelin}.
Hence, a $\lambda$-calculus needn't feature the whole operator $\bigC$,
while being strictly stronger than $\lambda_\to$.

Ariola and Herbelin in the paper present \emph{minimal classical logic}, which is strictly
weaker than classical logic but strictly stronger than minimal logic. Minimal classical
logic proves PL but does not prove DN, neither does it prove EFQ \cite{ariola-herbelin}.

We have seen that EFQ together with PL
is sufficient to prove DN, as demonstrated by expressing $\bigC$ in terms of $\bigA$ and
$\bigK$ in \Fref{tab:control-operators}. And as DN implies both EFQ and PL, the operator
$\bigC$ subsumes the other two, being able to redefine them both, which has also been
demonstrated in Table \ref{tab:control-operators}.

\begin{figure}
\centering
\begin{tikzpicture}[x=9mm,y=9mm,
        back line/.style={densely dotted,-},
        normal line/.style={-stealth,-},
        cross line/.style={normal line,-,preaction={draw=white,-,line width=6pt}},
        font=\small
    ]
	\draw
		(1,1)
		+(0.5,0) edge[back line] +(2.5,0)
		+(3.5,0) edge[back line] +(5.5,0)
		+(0,0.3) edge[back line] +(0,2.7)
		+(0.5,3) edge[normal line] +(2.5,3)
		+(3.5,3) edge[normal line] +(5.5,3)
		+(3,0.3) edge[back line] +(3,2.7)
		+(6,0.3) edge[normal line] +(6,2.7)
		;
	\draw[normal line]
		(0,0)
		+(0.5,0  ) edge +(2.5,0  )
		+(3.5,0  ) edge +(5.5,0  )
		+(0  ,0.3) edge +(0  ,2.7)
		+(0.5,3  ) edge[cross line] +(2.5,3  )
		+(3  ,0.3) edge[cross line] +(3  ,2.7)
		+(3.5,3  ) edge[cross line] +(5.5,3  )
		+(6  ,0.3) edge[cross line] +(6  ,2.7)
		;
	\draw
		(0,3) +(0.25,0.25) edge[normal line] +(0.7,0.7)
		(0,0) +(0.25,0.25) edge[back line] +(0.7,0.7)
		(3,3) +(0.25,0.25) edge[back line] +(0.7,0.7)
		(3,0) +(0.25,0.25) edge[back line] +(0.7,0.7)
		(6,3) +(0.25,0.25) edge[normal line] +(0.7,0.7)
		(6,0) +(0.25,0.25) edge[normal line] +(0.7,0.7)
		;
    \draw
    	(0,0)
    	+(0,0) node{ML}
    	+(1,1) node{IL}
    	+(1,4) node{CL}
    	+(0,3) node{MCL}
    	(3,0)
    	+(0,0) node{$\lambda_\to$}
    	+(1,1) node{$\lambda_{\bigA^-}$}
    	+(1,4) node{$\lambda_{\bigC^-}^\mathrm{top}$}
    	+(0,3) node{$\lambda_{\bigC^-}$}
    	(6,0)
    	+(0,0) node{$\lambda_\to$}
    	+(1,1) node{$\lambda^\mathrm{top}$}
    	+(1,4) node{$\lambda_\mu^\mathrm{top}$}
    	+(0,3) node{$\lambda_\mu$}
	    ;
	\draw[dotted,->,font=\footnotesize]
		(-1,0.5) -- (-1,2.5) node[left] {PL\,/\,$\bigK$};		
	\draw[dotted,->,font=\footnotesize]
		(7,0) -- (8,1) node[below right] {EFQ\,/\,$\bigA$};
\end{tikzpicture}
\caption{Logics of different ``classicality'', related to different $\lambda$-calculi}
\label{fig:classical-cube}
\end{figure}

From the other direction, Parigot created the $\lambda_\mu$ calculus \cite{parigot92}
as a way to assign computational content to classical natural deduction. The calculus
Parigot presented did not readily correspond to classical logic --
only to minimal classical logic -- but it can be extended to do so, yielding
a calculus named $\lambda_\mu^\mathrm{top}$, that matches classical natural deduction
operationally \cite{ariola-herbelin}.

\begin{comment}
\subsubsection{Parigot's $\lambda_\mu$ calculus}

The $\lambda_\mu$ calculus does not have control operators; instead, it extends
the $\lambda$-syntax by distinguishing between \emph{terms} and \emph{commands}
and introducing variables for continuations. A brief overview of the syntax
can be found in \Fref{fig:lambda-mu}; for proper definitions and a more thorough
discussion, the interested reader is referred to \cite{parigot92} or \cite{ariola-herbelin}.

The most interesting property for us is perhaps 

\begin{figure}
\centering
\begin{minipage}{0.75\textwidth}
Types:
\[ \rho, \sigma ::= \alpha \sbar \sigma \to \rho \]
Terms:
\[ r, s, t ::= x \sbar \lambda x : \rho.\, r \sbar s t \sbar \mu \alpha : \rho.\, c \]
Commands:
\[ c, d ::= [\alpha] t \]
Example terms:\\
\ughcenter{$\lam{x:\sigma} x$,\;\; $\lmu{\alpha:\rho} [\beta] \lam{y:\sigma} y$}
\end{minipage}
\caption{Syntax of $\lambda_\mu$}
\label{fig:lambda-mu}
\end{figure}
\end{comment}

Ariola and Herbelin studied all these calculi and proposed
slightly modified ones that have good metatheoretic properties (confluence, strong
normalization). They extended the $\lambda_\mu$ calculus to cover the whole classical
logic by introducing the top-level continuation constant named \emph{top}, also using this constant
in a weakened $\lC$ called $\lambda_{\bigC^-}$.
Their paper provides a schematic overview of the resulting $\lambda$-calculi, again in the
elegant form of a $\lambda$-cube, see \Fref{fig:classical-cube}.

\subsection{Exceptions versus control operators}

The similarity of behavior of control operators and exceptions hints at the possibility
to implement each of them in terms of the other. However, it is not completely
straightforward.

Indeed, exceptions can in some ways be simulated by control operators (and vice versa).
Consider the following definition of \ident{catch} operator, taking an expression
and a handler.
\[	e \;\midt{catch}\; h = \bigK(\lam{k} e (\lam{x} k(h x))) \]
This operator can then be used as follows:
\begin{align*}
	&(\lam{\midt{throw}} 2 + 3) \;\midt{catch}\; (\lam{e} 4 \cdot e) \red 5 \\
	&(\lam{\midt{throw}} 2 + \midt{throw}\,3) \;\midt{catch}\; (\lam{e} 4 \cdot e) \red 12
\end{align*}
However, there is a major catch\footnote{Pun intended.}. These exceptions are \emph{statically
(lexically) bound}: a throwing expression always refers to a very specific handler
by using the continuation named \ident{throw} to throw the value. This continuation
is tightly bound to the exact context of the catch-expression since it arose there
and was passed to the inner expression via a lambda.

In contrast, exceptions in standard programming languages are \emph{dynamically bound}, which
means that a thrown exception is handled by the innermost handler at the site of \emph{execution},
not \emph{definition}.

This is exactly the same difference as between dynamic and lexical \emph{variable} scoping. Some
languages use only dynamic variable scope (Bash), looking up the value of a variable by its name
in a run-time dictionary whenever the variable is used; some use only lexical variable scope (C),
where the memory cell of a variable can be determined syntactically at compile time; and some can
use both (Perl).

\begin{figure}
\centering
\begin{subfigure}{0.45\textwidth}
\begin{codepy}
  def f():
    try:
      return lambda x: raise x
    except:
      print "caught in f()"
      return lambda x: x
      
  g = f()
  try:
    g(0)
  except:
    print "caught at toplevel"
\end{codepy}
\caption{Python}\label{fig:svd-python}
\end{subfigure}
\begin{subfigure}{0.45\textwidth}
\[ \big((\lam{t} t)\;\midt{catch}\;(\lam{e} 1 + e)\big)\, 1 \]
\caption{$\lambda$-calculus}\label{fig:svd-lc}
\end{subfigure}
\caption{Example programs: lexical vs. dynamic binding}
\end{figure}

To illustrate the difference between these two approaches, consider the
Python\footnote{Actually, almost-Python. In Python,
\ident{raise} is a statement and as such it cannot be used in lambdas. However, defining
a nested function would introduce unnecessary syntactic noise here so let us leave this as it is.}
program in \Fref{fig:svd-python}. Every Python programmer would expect it to print
``caught at toplevel'' -- and this is exactly what happens with dynamically bound exceptions.
The exception is thrown at the point of calling \ident{g(0)} so it is caught by the corresponding
top-level handler.

However, with lexical scoping, the value \ident{g} would carry also the continuation needed
to reinstate execution at the point of the handler inside the function \ident{f}. In such cases,
when the function \ident{g} is called, it throws an exception and execution jumps to the handler
\emph{inside \ident{f}}, printing ``caught in \ident{f()}''. This contradicts the general
understanding of exceptions among programmers.

This difference has been covered in multiple papers, most notably by Thielecke, who
points out the contrast between exceptions and control operators very explicitly
\cite{thielecke:contrast}. Thielecke also analyzes the nature of the relationship between
them, using a specialized CPS transform to unify and generalize these two approaches
\cite{thielecke01}. In the latter paper, Thielecke also implements a pseudo-\ident{call/cc}
using dynamically bound control operators and argues that it has only an intuitionistic type.

In the former paper \cite{thielecke:contrast}, Thielecke also presents how exceptions,
control operators\footnote{Thielecke uses the word ``continuations'' to denote this kind
of control.} and mutable state interact and proves several interesting no-go theorems:
exceptions cannot express control operators (even when augmented with state); control
operators in the absence of state cannot express exceptions; exceptions and control operators
together cannot express state.

Therefore unfortunately, despite some resemblance, we cannot expect that adding exceptions
to a language will strengthen the corresponding logical system, although it may well appear so.

\subsubsection{Peirce's law and $\lambda_\mu$}

To illustrate where this resemblance breaks, let us take an example: Peirce's law, 
proof of which is
expressible in $\lambda_\mu$ \cite{krebbers11}\footnote{Church-style type annotations
in lambdas have been omitted for brevity.}. This proof is a good example because while
it has a direct
control-operator counterpart $\bigK$, it can also be expressed using just \ident{throw}
and \ident{catch}. We will exploit this property to demonstrate the difference between
statically and dynamically bound exceptions. For reference on $\lambda_\mu$ see
\cite{parigot92} or \cite{ariola-herbelin}.

Let us look at the proof term of Peirce's law in $\lambda_\mu$.
\[ \Gamma; \Delta \vdash_\mu 
    \big(\lam{t} \lmu{\alpha} [\alpha] t (\lam{x} \lmu{\beta} [\alpha] x) \big)
      : ((\rho \to \sigma) \to \rho) \to \rho
\]

An expression $\lmu{\alpha}[\beta]t$ can be viewed as a combined catch-throw, catching
exceptions labeled $\alpha$ from $t$, or throwing the result of $t$ with the label $\beta$
(depending on whether $t$ throws or not). The expression $\lmu{\alpha}[\alpha]t$ catches
from $t$ and throws only to itself so it is a natural \ident{catch}$_\alpha$. On the
other hand, the expression $\lmu{\alpha}[\beta]t$, where $\alpha$ does not occur free in
$t$, cannot catch anything, thus representing a \ident{throw}$_\beta$ expression.

The above proof of Peirce's law can therefore be rewritten as:
\[ \bigP = \lam{t} \midt{catch}_\alpha \big(t (\lam{x : \rho} \midt{throw}_\alpha\, x)\big) \]
In the second lambda, an explicit type annotation was added to emphasize the type of $x$.

This form of the proof can be interpreted as follows. Suppose we have a value
$t : (\rho \to \sigma) \to \rho$. Then we need to produce a value of the type $\rho$.
We clearly cannot do that outright because $\rho$ is arbitrary and may be even empty.

However, the type of $t$ says that $t$ will supply such a value, given that we can provide
it with a function of the type $\rho \to \sigma$. We don't readily have such a function either;
it would have to return a $\sigma$ and we don't have any $\sigma$ to construct the function with.

Nevertheless, with exceptions, we \emph{can} produce a function of the type $\rho \to \sigma$
because we can make the function always throw an exception and never return, which enables us
to give it an arbitrary return type, especially $\sigma$. What's more, this function
takes an argument of the type $\rho$ so the values it throws can have the type $\rho$.

So the final plan is as follows: within a catch$_\alpha$-block, apply the function $t$ to
$\lam{x} \midt{throw}_\alpha x$. The function $t$ either uses its argument or not.

If it does, since
the only way to use a functional argument is to apply it to a value of the
appropriate type, in our case $\rho$, the value of the type $\rho$ gets thrown to the
\ident{catch}$_\alpha$ and it is returned directly.

If it does not, the returned value does not depend on the function we provided, the
exception is never triggered and a value of the type $\rho$ is returned normally from
the function $t$.

Whichever the case, we have succesfully obtained a value of the type $\rho$ to return, which
makes our term a proof of Peirce's law -- and it is probably no surprise that it behaves
exactly like the control operator $\bigK$, representing \ident{call/cc} from Scheme.

\subsubsection{Peirce's law and statically bound exceptions}

However, this only works because exceptions in $\lambda_\mu$ are statically bound,
as already mentioned:
consider the usage of the proof of Peirce's law in this -- rather elaborate --
identity function\footnote{In this example, we write
explicit type annotations wherever possible to help clarify the idea.}.
\[ \bigI = \midt{catch}_{\alpha : \rho \to \rho} \big(
	  \lam{x : \rho} \bigP(\lam{f : \rho \to \rho} \midt{throw}_{\alpha: \rho\to\rho} \, f)
   \big)
\]
It can be shown that in the $\lambda_\mu$ calculus, the above definition can be given
the type $\rho \to \rho$ for any type $\rho$ (see the appendix, \Fref{fig:lambda-mu-identity})
\label{sec:peirce-law-dynamic}. Furthermore, due to parametricity, it must define the identity
function \cite{wadler89}.

Let us fix $\rho = \bN$ and see how an application of $\bigI$ to $0$ reduces:
\[ \bigI 0 \red ? \]
In a lexical setting, the above term undergoes capture-avoiding substitution,
first reducing to
\[ \midt{catch}_{\alpha : \rho \to \rho} \Big[
	  \lam{x}
	  	\Big(\lam{t} \midt{catch}_{\alpha : \rho}
	  		\big(t (\lam{x : \rho} \midt{throw}_{\alpha : \rho}\, x)\big)\Big)
	  	(\lam{f : \rho \to \rho} \midt{throw}_{\alpha: \rho\to\rho} \, f)
   \Big]\,0\text{,}
\]
and then substituting to
\[ \midt{catch}_{\alpha : \rho \to \rho} \Big[
	  \lam{x}
	  	\midt{catch}_{\beta : \rho}
	  		\big((\lam{f : \rho \to \rho} \midt{throw}_{\alpha: \rho\to\rho} \, f) (\lam{x : \rho} \midt{throw}_{\beta : \rho}\, x)\big)
   \Big]\,0\text{,}
\]
reducing to
\[ \midt{catch}_{\alpha : \rho \to \rho} \Big(
	  \lam{x} \midt{catch}_{\beta : \rho}
	  		\big(\midt{throw}_{\alpha: \rho\to\rho}\,
	  		  (\lam{x : \rho} \midt{throw}_{\beta : \rho}\, x)
	\big)\Big)\,0\text{,}
\]
and finally, after a chain of reductions, indeed reducing to
\[
	0 \text{,}
\]
according to the corresponding reduction rules \cite{ariola-herbelin, krebbers11, parigot92}.
Note that the $\alpha$ labeling the inner \ident{catch} is distinct
from the outer $\alpha$; this was exhibited by the need to rename the inner $\alpha$
to $\beta$ in order to avoid name collisions.

\subsubsection{Peirce's law and dynamically bound exceptions}

The above combinator $\bigP$ can be translated to a language with dynamically bound
exceptions. The intended behavior of the combinator may be observed in many cases.
It does nevertheless not hold in general since we lose the capture-avoiding substitution
and statical context/continuation references
from $\lambda_\mu$. As the calculus where we will test our $\bigP$ and $\bigI$ combinators,
let us pick OCaml.

The program containing the definitions of $\bigI$ and $\bigP$, along with its output,
can be seen in \Fref{fig:ip-ocaml}. Note that the OCaml expression \ident{i 0} no longer
reduces to $0$; instead, it throws an exception. What happens there?

\begin{figure}
\centering
\begin{subfigure}[b]{0.55\textwidth}
\begin{codeml}
exception E1 of int
exception E2 of (int -> int)

let p : (('a -> 'b) -> 'a) -> 'a =
  fun t ->
    try t (fun x -> raise (E1 x))
    with E1 x -> x

let i : ('a -> 'a) =
  try fun x -> p (fun f -> raise (E2 f))
  with E2 f -> f

let _ =
  print_int (i 0)
\end{codeml}
\caption{The test program}
\end{subfigure}
%
\begin{subfigure}[b]{0.35\textwidth}
\begin{verbatim}
$ ocaml example.ml
Exception: E2 <fun>.
$
\end{verbatim}
\caption{Output of the program}
\end{subfigure}
\caption{The combinators $\bigP$ and $\bigI$ in OCaml, along with the program output}
\label{fig:ip-ocaml}
\end{figure}

It turns out that when the function \ident{i} is called with the parameter $0$, it reduces
the lambda within the try-block. The lambda reduces to itself, throwing no exceptions,
so it is simply returned and the handler for \ident{E2} is ignored.

Hence we get a term that still contains ``\ident{raise (E2 f)}'' as a subterm \emph{outside}
of the scope of the lexically superior exception handler. The remaining machinery
with the combinator \ident{p} just causes that the exception gets indeed raised -- and since
there is (dynamically) no handler left that could catch this exception, it propagates to the
toplevel and aborts the program with an error message.

In a language with statically bound exceptions, the subterm ``\ident{raise (E2 f)}'' would
always refer to the lexically superior handler within the function \ident{i}, along
with the corresponding execution context. Therefore, when the exception \ident{E2} gets thrown,
it does not search dynamically for an exception handler, not finding it and aborting.
Throwing this exception directly reinstates execution at the point of the exception handler,
using the original execution context.

Part of the surprise of the OCaml program stems from the fact that
there is no way to tell from the type of a function what exceptions this
function might throw. This is addressed in a $\lambda$-calculus with dynamically bound
exceptions presented by Kameyama \cite{kameyama:dynamic}. This calculus alters the notation
of functional types: these are written $A \stackrel{\Delta}{\to} B$, where $\Delta$ denotes
the set of the possible types for values thrown from the function.

In our example, using such an annotation would reveal that the function \ident{i} might
throw exceptions despite the fact that all \ident{throw} statements are lexically closed by
their corresponding handlers.

\subsection{Conclusion}

Hence the theoretical appeal of studying exceptions in functional languages lies in the wonderful
world of different logical systems related to control operators. Although exceptions,
as usually presented, do not have the power of statically bound control operators,
in certain contexts, they behave very similarly and parallels can definitely be drawn.

This also means that when studying exceptions in practice, either semantics or implementation,
looking at ``how the guys from Scheme do it with their control operators''
may be beneficial because the principles are shared on many levels and despite their
occasionally completely different behavior, they can still be described by a single construct
with a trivial difference \cite{thielecke01}.

\section{Practical appeal}

\todo{Appeal of certified compilers.}

\todo{Better definition of $\lambda_\mu$?}

\label{chap:dependent-types}






































































