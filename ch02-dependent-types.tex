\chapter{Dependently typed programming in Agda}
\label{chap:dependent-types}

We will create our development in a dependently typed pure functional language named Agda.
This chapter provides a brief introduction to Agda, along with some commentary about
related topics and also presents reasons why Agda was chosen as the implementation language.

\todo{Does it present the reasons?}

The aim is to provide the bare necessities for reading this thesis if the reader is not
familiar with Agda or dependently typed programming -- mostly an overview of Agda syntax and some
basic techniques used.

\todo{Not only language but also logic. Per Martin-L\"{o}f's Type Theory? Dependent types. 
The Curry-Howard correspondence. Total functional programming.
Structural recursion and termination. Formal verification. The Agda proof assistant}

\section{A simple exceptionless language}
\label{sec:simple-language}

As a brief Agda crash-course, we will implement a very simple compiler for a
language of arithmetical expressions to instructions for a stack machine.

Being a simple and instructive task, this has been done many times in a variety
of programming languages in other papers, publications, blog posts, et cetera;
for example in \cite{epigram-compiler} (in Epigram) or in \cite{chlipala:compiler}
(in Coq).

All high-level languages in this thesis will be languages of simple, typed
expressions and our first language will feature only natural numbers, addition,
and no exceptions. Besides demonstrating how dependent programming in Agda
looks like, we will also implement it to build the auxiliary ecosystem of the
compiler and the skeleton of our development, upon which we will build later
throughout the thesis.

\todo{Add a reference to the source code.}

% branch: no-exceptions

\subsection{Type universe}

It is probably a reasonable requirement that our language should be able to represent
expressions of different types (e.g. numeric or Boolean).

Hence the first thing we will introduce is the type universe representing the set of
types of expressions of our high-level language. We will \emph{index} our types with
values of this type, most notably the type \ident{Exp} of expressions, to
indicate what the type of the expression is.

While we could use Agda
types directly for indexing, with an explicit universe, we get decidable
equality of types and all datatypes conveniently in \ident{Set}.%
\footnote{To avoid cardinality trouble, reified in Agda as Girard's Paradox
\cite{girard:dissertation} (simplified by Hurkens in \cite{hurkens}),
Agda features stratified type universes with universe polymorphism.

Our approach, modeling the types of our expression language as values
of the universe \ident{U} together with an interpretation function,
lets us have all types flat in the lowest Agda type
universe, called \ident{Set}, relieving us from the burden of caring about the
stratification.}

\note{In this thesis, we will be a bit lax on wording related to type
universes.  We will be talking about ``expressions of the type \ident{u}'',
while actually referring to ``terms that represent expressions of \emph{the
type denoted by \ident{u}}''. This simplified approach can hardly cause
confusion, while adhering to precise wording in every situation at all costs
would sacrifice comprehensibility.}

\subsubsection{Data type declaration}

Here we arrive at Agda \emph{type declarations}. These resemble GADTs from
Haskell or type declarations from other dependently typed languages, like Coq.
The first line contains the name of the data type and the universe we want to
put the type in. The following lines contain data constructor declarations,
including an explicit type for each data constructor.

As a general rule, Agda uses indentation instead of punctuation to delimit
blocks. Hence the constructors \emph{must} be indented; in return we get
visually clean code.

\begin{code}
  data U : Set where
    nat : U
\end{code}

\noindent This universe could not be much simpler. While we aim to support more
types at a later stage, now we restrict the language to express only natural
numbers, for the sake of simplicity. As long as we build the program to
distinguish different types, this is just a quantitative difference.

Being so simple, this is hardly a representative example of a data type
declaration. A bit more elaborate example follows shortly in \Fref{sec:simple-exp}.

\subsubsection{Interpretation function}

We will also need an interpretation function that maps
types of our simple language to Agda types so that we can use Agda values in
the modelled language, talk about its denotational semantics etc.

Function declarations in Agda consist of a type declaration and a (possibly multiple)
number of clauses describing the outcomes of the function depending on
its arguments.

\begin{code}
  el : U -> Set
  el nat = Nat
\end{code}

\noindent There is one clause/equation per distinguished combination of arguments. If the
universe \ident{U} contained another value \ident{bool : U}, the function \ident{el}
would contain another clause in the form:
\begin{code}
  el bool = Bool
\end{code}
Users of languages belonging (syntactically) to the Miranda family will feel safely at
home here; the difference to ML-style languages here is that instead of a big match expression
on the right side of a single equation, there are multiple equations where pattern
matching takes place on the left side of each equation.\footnote{Seen for example in
the definition of the function \ident{denExp} in \Fref{sec:simple-denExp}.}

Furthermore, beyond standard type checking etc., Agda, being a \emph{total} functional
programming language, performs two special checks on every function definition.
\begin{itemize}
	\item The \emph{coverage checker} checks that pattern coverage of every
		function definition is exhaustive. In other words, whatever arguments
		are given to the function, \emph{some} pattern from its definition must match
		them.
	\item The \emph{termination checker} checks that recursion used (if any) by the
		given function is structural and hence this function is obviously terminating.
\end{itemize}
This ensures that every function terminates on every input, which is important for soundness
of proofs. This will be discussed later.

\todo{Don't forget to discuss it and insert a reference here.}

\subsection{Expressions}

The core of the language we are going to model consists of its expressions, of
course. For now, we will support nothing more than (numeric) literals and addition.
However, for further extensibility, we separate the type of binary operators.

\subsubsection{Operators}

The type of binary operators is indexed by the types of the two values that
the operator accepts as arguments; the third index represents the type of
the result of application of the operator on the two values.

\begin{code}
  data Op : U -> U -> U -> Set where
    Plus : Op nat nat nat
\end{code}

\noindent Put another way, a value of the type \ident{Op u v w} represents
a binary operator whose operands have the types \ident{u} and \ident{w},
and whose result has the type \ident{w}. Hence, the value \ident{Plus}
represents an operator that takes two \ident{nat}s and returns a \ident{nat}.%
\footnote{We will add more operators later. How the constructors representing
different operators are typed can be seen in \Fref{sec:more-operators}.}

\subsubsection{Expressions}

Now we can define the expressions: literals and binary operators.
Note that also this data type is indexed with elements of the type universe
\ident{U} so that the Agda type of an expression also incorporates the type of
the expression in the modelled language.

\begin{code}
  data Exp : U -> Set where
    -- Literals
    Lit : forall {u} -> el u -> Exp u
    -- Binary operators
    Bin : forall {u v w} -> Bin u v w -> Exp u -> Exp v -> Exp w
\end{code}\label{sec:simple-exp}

\noindent This is a bit more elaborate declaration using several Agda features.
\begin{itemize}
	\item Agda supports \emph{Unicode} in its source files and is very liberal about what
		characters may occur in tokens. This enables Agda source code to get much closer
		to standard math notation. Hence the $\forall$ and $\to$ characters
		are contained literally in it.
		
		What's more, Agda does not distinguish tokens as operators or identifiers
		on a lexical basis; (almost) anything may be an infix or mixfix operator,
		if properly declared\footnote{Discussed in \Fref{sec:fixity}.} and ordinary
		identifiers may consist purely of special characters.
		This also means that operators must be surrounded by whitespace: for example,
		``\ident{foo-bar}'' parses as a single identifier containing a hyphen.
		
	\item The names enclosed in braces are \emph{implicit arguments}, as usual
		in dependently typed languages. For example, a fully saturated application of the
		constructor \ident{Lit} contains only one argument; the implicit argument
		\ident{u} is inferred from the context of the application.
		
	\item The $\forall$ sign in front of the implicit arguments enables us to \emph{leave
		out the types} of the arguments -- these will be inferred from the context of
		the constructor declaration. An equivalent declaration of the constructor \ident{Lit}
		would be \ident{Lit : \{u : U\}  $\to$ el u $\to$ Exp u}.
		This also works with explicit (non-braced) arguments.

	\item The implicit arguments above are \emph{named}. Explicit arguments can be named, too;
		we could well write \ident{Lit : $\forall$ \{u\} $\to$ (x : el u) $\to$ Exp u}.
		However, we do not use the name \ident{x} anywhere, unlike the name \ident{u},
		so we don't need to even create it.
		
	\item Note that we \emph{apply a function in the declaration} of the constructor
		\ident{Lit}. In a language with dependent types, we can use function application
		in type declarations freely.
\end{itemize}

Given the above definition of the type of expressions, it can be immediately seen that
literals of our expression language always carry appropriately typed values of the
type \ident{el u}.

The typing machinery also ensures that binary operators receive operands of correct
types, yielding an expression typed exactly as given by the operator type specification.

\subsection{Semantics of expressions}

Our definition of the high-level language would not be complete without giving
the denotational semantics of its expressions. This is done in the following
pair of simple functions.

\subsubsection{Operator semantics}

Semantics of operators is given by the function \ident{denOp} as follows.

\begin{code}
  denOp : forall {u v w} -> Op u v w -> el u -> el v -> el w
  denOp Plus = _+\_
\end{code}

\noindent The function \ident{denOp} can be interpreted as taking a value that represents
a binary operator of the type \ident{Op u v w} and returning an appropriately-typed Agda
function of the type \ident{(el u $\to$ el v $\to$ el w)}.

The function returned for the operator \ident{Plus} is the ordinary addition function
from the standard library. Surrounded with underscores, the infix operator \ident{+} becomes
a standard function identifier.\footnote{See \Fref{sec:fixity} for more information on
\textvisiblespace\hspace{0.8pt}fix operators in Agda.}

\subsubsection{Expression semantics}

Expressions are then turned into Agda values as follows; literals in a trivial
way, binary-operator expressions using the denotation of the corresponding
operator and recursively obtained denotations of the operands.

\begin{code}
  denExp : forall {u} -> Exp u -> el u
  denExp (Lit x) = x
  denExp (Bin op l r) = denOp op (denExp l) (denExp r)
\end{code}\label{sec:simple-denExp}

\noindent As already mentioned, pattern matching happens on the left side of defining equations,
it is \emph{exhaustive}: both of the two possible constructors are covered; and recursion
is structural: both recursive applications are made to a subterm of the argument of the function.
Hence this function is total and provably terminating.

\subsection{Virtual machine}

We will use a very simple machine to run the compiled code, featuring only a stack
of values.

\subsubsection{Stack}

The stack of the machine is just a cons-list of values, indexed by the list
of types (elements of the universe \ident{U}) of the values pushed on the stack.
This means that just by looking at the type of the stack, we can tell how many
elements it contains and what types they have. Let us first define the type
of stack shapes.
\begin{code}
  -- open import Data.List
  infixr 5 _::_
  data List (a : Set) : Set where
    [] : List a
    _::_ : a -> List a -> List a
\end{code}
\begin{code}
  Shape : Set
  Shape = List U
\end{code}
\label{sec:fixity} In the definition of the standard list data type above,
we encountered a declaration of an infix cons constructor. While it is true that
an infix operator surrounded by underscores becomes an ordinary identifier,
Agda goes much further and permits (almost) arbitrary prefix, infix, postfix
and mixfix operators with an arbitrary number of underscores.

Declaring an identifier containing underscores modifies the Agda parser
to recognize occurrences of that identifier where the underscores have been
replaced by subexpressions. Combined with Unicode support and the (practical) absence
of lexical rules, this is a very powerful device. A few examples (some coming
from the standard library) can be seen
in \Fref{tab:mixfix}.

\begin{table}[htp]
\centering
\begin{tabular}{lll} \toprule
\textit{Mixfix form} & \textit{Applicative form} & \textit{Description} \\ \midrule
\ident{x :: xs}		& \ident{\_::\_ x xs} 			& standard infix \\
\ident{x !}			& \ident{\_! x}					& postfix factorial \\
\ident{$-$[ x +1]}	& \ident{$-$[\_+1] x}	& negative whole number constructor \\
\ident{$\langle$ 2 * x $\rangle$} & \ident{$\langle\_\rangle$ (2 * x)}& non-trivial constructs work fine \\
\ident{x + 1 * y}	& \ident{\_+\_ x (\_*\_ 1 y)}	& fixity and precedence work as defined \\
\ident{if x then y else z} & \ident{if\_then\_else\_ x y z} & mixfix with non-symbols \\
\ident{x $\lhd$ $\varepsilon$} & \ident{$\_\!\lhd\!\_$ x $\varepsilon$} & unicode \\
\bottomrule
\end{tabular}
\caption{Agda mixfix operator examples}
\label{tab:mixfix}
\end{table}

The declaration \ident{\infixr\ 5 \_::\_} gives fixity and precedence for the operator.
The higher the number, the stronger the operator binds. Associativity is then given
by the variant of the declaration: \ident{\infixr} declares a right-associative operator,
\ident{\infixl} declares a left-associative operator, \ident{\infix} declares a non-associative
operator.

Let us return to the compiler. Having defined the type of stack shapes, we can
proceed to a definition of the type of stacks.

\begin{code}
  infixr 5 _\scons\_
  data Stack : Shape -> Set where
    snil : Stack []
    _\scons\_ : forall {u s} -> el u -> Stack s -> Stack (u :: s)
\end{code}
The literal \ident{snil} represents the empty stack; new values are
pushed onto it using the infix constructor \ident{\bin{\scons}}.

Note that pushing a value on the stack changes the type of the stack: the shape
index gets prefixed by the type of the value pushed. Given that the empty stack
is indexed by the empty shape, we always know how many items there are on the
stack and what types they have, as already mentioned above.

\subsubsection{Instructions}

At this stage, the machine supports only two instructions: \ident{PUSH}
and \ident{ADD}. This gives rise to the following data type of instructions.
\begin{code}
  data Instr : Shape -> Shape -> Set where
    PUSH : forall {u s} -> el u -> Instr s (u :: s)
    ADD : forall {s} -> Instr (nat :: nat :: s) (nat :: s)
\end{code}
The type of instructions is indexed by their action on the stack. The first shape
argument is the required stack shape so that the instruction can be executed;
the second shape argument is the resulting shape of the stack after the
instruction has been executed.

For example, the instruction \ident{PUSH} takes any value of the type \ident{el u}
and pushes it onto a stack having any shape \ident{s}, creating a new
stack of the shape \ident{u :: s}.

The instruction \ident{ADD} represents popping two natural numbers from the
stack of any shape with two \ident{nat}s on top of it, (hence
\ident{nat :: nat :: s})
and subsequently pushing their sum onto it, resulting in the shape
\ident{nat :: s}.

\subsubsection{Code}

Finally, code for the stack machine is a sequence of instructions, where
type indices of subsequent instructions match. For example, if one instruction
in the sequence produces a stack of the shape \ident{nat :: nat :: s},
we want the next instruction in the code sequence to accept this shape.

If we regard
\ident{Instr : Shape $\to$ Shape $\to$ Set}
as a binary relation on \ident{Shape}, then code is the \emph{transitive reflexive closure}
of \ident{Instr}, which is already included in the Agda standard library as the
module \ident{Data.Star}.

\begin{code}
  -- require import Data.Star
  infixr 5 _<|\_
  data Star {a b : Set} (R : a -> b -> Set) : a -> b -> Set where
    \nil : forall {x} -> Star R x x
    _<|\_ : forall {x y z} -> R x y -> Star R y z -> Star R x z
\end{code}

\begin{code}
  Code : Shape -> Shape -> Set
  Code = Star Instr
\end{code}

\noindent The type of instruction sequences is indexed in exactly the same
manner as the type of separate instructions: the first index represents the
acceptable shape of stack before execution of the piece of code; the second
index represents the shape of stack after its execution.

Let us conclude this section with an utility function for concatenation of
instruction sequences, which is actually also included in \ident{Data.Star}.

\begin{code}
  infixr 5 _\app\_
  _\app\_ : forall {R x y z} -> Star R x y -> Star R y z -> Star R x z
  \nil \app ys = ys
  (x <| xs) \app ys = x <| xs \app ys
\end{code}

\subsection{Execution}

Now we will describe how the machine executes instructions, that is,
the operational semantics of the low-level language.

At this stage, the state of the machine is fully described by just its stack. This
means that there are no other state variables, registers or any additional
memory.

\subsubsection{Instructions}

First, we describe the effects of single instructions on the state of the machine,
that is, on the stack, separately.

\begin{code}
  execInstr : forall {s t} -> Instr s t -> Stack s -> Stack t
  execInstr (PUSH x) st = x \scons st
  execInstr ADD (x \scons y \scons st) = (x + y) \scons st
\end{code}

\noindent The above function simply says that
\begin{itemize}
  \item the effect of the instruction \ident{PUSH} is pushing the attached
    value onto the stack. This consistently extends the information contained
    in the type of \ident{PUSH x}.\footnote{The type is \ident{Instr s (u $::$ s)}
    -- for some \ident{u} and \ident{s} and is interpreted as ``\ident{PUSH x}
    pushes some value of type \ident{u} onto the stack''.}
    What the type does not say (and \ident{execInstr}
    does) is what this value exactly is.
  \item the effect of the instruction \ident{ADD} is popping two \ident{nat}s from
    the top of the stack and pushing their sum back.
\end{itemize}

Note that in this definition of the execution function, we already reap some
benefits of dependently typed programming.

First, of course, Agda checks types of the terms behind
the scenes and the machinery of types we have designed so far ensures that
in the case for \ident{PUSH x}, pushing the value \ident{x} always yields
a stack of the desired shape.

Second, in the case for \ident{ADD}, the types ensure that there are always two
\ident{nat}s on top of the stack and we can safely pattern-match with the
pattern \ident{x} \scons \ident{y} \scons \ident{st} -- because this match
will always succeed (and no other patterns for the \ident{ADD} case are needed). 

Thus the above definition complies to the type signatures involved (relatively
solid hints of correctness) and it is \emph{total} (esp. no pattern match failures),
while compilers of non-dependently typed languages, like OCaml or Haskell,
would complain about non-exhaustive patterns here --- there is no way to tell them
that, for example, we needn't deal with empty stacks when executing \ident{ADD}.

\todo{There's a paper saying that non-exhaustive matches are inevitable; insert
a reference.}

\subsubsection{Code}

Execution of code is then just a left fold over the sequence of instructions,
accepting the initial and yielding the resulting state of the machine.

\begin{code}
  execCode : forall {s t} -> Code s t -> Stack s -> Stack t
  execCode \nil st = st
  execCode (i <| is) st = execCode is (execInstr i st)
\end{code}

\noindent Execution of empty code has no effect on the stack; if the code
contains instructions, then the first instruction is executed and on the
resulting stack, the rest of code is executed.

Note that the type signature of \ident{execCode} ensures that the code being
run modifies (the shape of) the stack consistently with its type indices.

\subsection{Compiler}

Compiling our simple high-level language for a stack machine is easy. The
central idea is that execution of an expression of some type is equivalent to
pushing its value onto the stack. Literal values are then pushed on the stack
directly; binary-operator expressions first evaluate both operands, effectively
putting their values on the top of the stack, and then execute the appropriate
instruction, determined by the operator. This instruction pops the top
two values from the stack as its operands and pushes the result back.

\label{sec:simple-compiler}\begin{code}
  -- Syntactic sugar, promote an &\cident{Instr}& to singleton &\cident{Code}&
  [[_\;]] : forall {s t} -> Instr s t -> Code s t
  [[i\;]] = i <| \nil

  -- Determine what instruction performs the required calculation
  opInstr : forall {u v w} -> Op u v w -> forall {s} -> Instr (u :: v :: s) (w :: s)
  opInstr Plus = ADD

  -- Turn the expression into code
  compile : forall {u} -> Exp u -> forall {s} -> Code s (u :: s)
  compile (Lit x) = [[ PUSH x ]]
  compile (Bin op l r) = compile r \app compile l \app [[ opInstr op ]]
\end{code}

\noindent Again, behind the scenes, Agda ensures that all types match and the
code compiled by this function will not make the stack machine fail.\footnote{
  To be fair, this is already a property of \ident{Code}, ,,inherited'' by the
  function \ident{compile} via its return type.  However, it does constrain
possible definitions of the function \ident{compile}.} For example, there is no
way to have the function \ident{compile} output code where \ident{ADD} would
not get two \ident{nat}s on the top of the stack.

\subsection{Correctness}

This is the only place in this thesis where we include the full proof
of correctness. All proofs are of course contained in the attached Agda source
code.

\subsubsection{Agda as a proof assistant}

Apart from being a total dependently typed functional \emph{programming} language,
the ``total dependently typed'' part also makes Agda suitable for proving theorems.
This means that type signatures can express theorems, and values of these
types correspond to their proofs. Hence, type checking coincides with proof checking.

\todo{Insert references to further explanation, Curry-Howard etc.}

\subsubsection{Propositional equality}

In the following proofs, we will use the operator $\equiv$ to denote
\emph{propositional equality}. This is realized in Agda through the following
type family.

\begin{code}
  data _==_ {a : Set} (x : a) : a -> Set where
    refl : x == x
\end{code}

\noindent The definition implies that all nonempty members (inhabited by \ident{refl}) of this
family must have the index identical to the parameter.
Therefore conversely, if we have a \ident{refl : a == b}, it must be the case that \ident{a}
is identical to \ident{b}.

This identity is taken into account by Agda when doing pattern matching on the constructor
\ident{refl} and causes unification of the corresponding type variables. This is not special
to propositional equality at all -- after all, propositional equality is expressed by
an ordinary type family -- it is just a consequence of a more general unification mechanism,
which works this way for any other data type.

\subsubsection{Operator lemma}

There are two auxiliary lemmas that we will need to prove our main result. The
first one of them is called \ident{op-correct} and it says that for any binary
operator, the instruction picked by the compiler indeed does what the
denotation of the binary operator says.

To be more specific, for any operator \ident{op} and two values \ident{x} and
\ident{y} of appropriate types, executing \ident{opInstr op} with the two
values on top of the stack results in having the value \ident{denOp op x y}
on the top of the stack afterwards.

\label{sec:cor-op-correct}\begin{code}
  op\-correct : forall {s u v w} {st : Stack s} {x : el u} {y : el w}
    -> (op : Op u v w)
    -> execInstr (opInstr op) (x \scons y \scons st) == denOp op x y \scons st
  op\-correct Plus = refl
\end{code}

\noindent In the case for \ident{Plus}, Agda substitutes the term \ident{Plus}
for the variable \ident{op} in the appropriate places in the equality, normalizes
it (expanding function definitions etc.) and the proof becomes a trivial observation of
identity of normal forms, which is indicated by \ident{refl}.

\subsubsection{Distributivity lemma}

The other lemma that we will need says that execution of code distributes over
concatenation of code. In other words, executing the code \ident{c $\lhd\!\lhd$
d} has the same effect as first executing \ident{c} and then executing
\ident{d} on the resulting stack.

\label{sec:cor-compile-distr}\begin{code}
  compile\-distr : forall {s t u} {st : Stack s}
    -> (c : Code s t) -> (d : Code t u)
    -> execCode (c \app d) st == execCode d (execCode c st)
\end{code}

\noindent We will proceed by induction on the parameter \ident{c}, which yields
two cases: either \ident{c} is empty or it consists of an instruction and the
rest of code. The first case is trivial by substituting $\varepsilon$ for the variable
\ident{c} in the equality and observing identity of the normal forms on both sides.

\begin{code}
  compile\-distr \nil d = refl
\end{code}

\noindent For writing the proof for the second case, we will use the wonderful
way supported by the Agda module \ident{$\equiv$-Reasoning}\footnote{Actually,
there are also other similar modules, like \ident{$\le$-Reasoning} etc.}, which
lets us write proofs in the equational-reasoning style; appearing just the way
it would if we did it with pen and paper.\footnote{This is achieved by clever
mixfix hackery and Unicode usage.}

\begin{code}
  compile\-distr (i <| is) d = let open ==\-Reasoning in begin
    execCode (i <| is \app d) st
      ==< refl \>
    execCode (is \app d) (execInstr i st)
      ==< compile\-distr is d \>
    execCode d (execCode is (execInstr i st))
      ==< refl \>
    execCode d (execCode c st)
    \qed
\end{code}

\noindent The proof begins with the first line, which is usually exactly the
left-hand side of the equality we aim
to prove.  The second line contains the proof that the first line is equal to
the third line and so on -- by alternating terms and equality proofs, we can
gradually rewrite the left-hand term to the right-hand term of the desired
equality.

The first proof is just comparison of normal forms, as indicated by
\ident{refl}. In this step we just unfold the definition of \ident{execCode},
immediately obtaining the next term.

The second proof uses \ident{compile-distr} recursively as an induction
hypothesis to break the execution of concatenated code into two stages: first
executing \ident{is}, then executing \ident{d}.

The third proof is just \ident{refl} again and we use it to restructure the
term to the desired final form, this time \emph{folding} the longer
subterm to its definitional equivalent \ident{execCode c st}.

\subsubsection{A shorter proof of distributivity}

Since \ident{refl} is a certificate of identity of normal forms, by rewriting
a term to a diferent form using \ident{refl} as the proof, the normal form of
that term remains the same. As normal forms is what Agda compares when typechecking,
we can omit the intermediate \ident{refl}-based rewrites, which leaves us
with just one proof in the chain.\footnote{Since a chain consists of equality
proofs connected with transitivity of equality, a singleton chain is identical
to the single proof itself.}

\begin{code}
  compile\-distr : forall {s t u} {st : Stack s}
    -> (c : Code s t) -> (d : Code t u)
    -> execCode (c \app d) st == execCode d (execCode c st)
  compile\-distr \nil d = refl
  compile\-distr (i <| is) d = compile\-distr is d
\end{code}

\noindent However, often human readability is more desirable than terseness of
the code and the equational proof may be more appropriate.

\subsubsection{Main correctness theorem}

This is the central result of this stage that relates together everything we
have defined so far in a single proof of correctness.

This proof formalizes the idea that we informally mentioned when we started to
write the compiler: executing the compiled code for an expression should be
equivalent to pushing the value of the expression (as given by the denotational
semantics) onto the stack.

\label{sec:cor-correctness}\begin{code}
  correctness : forall {u s}
    -> (e : Exp u) (st : Stack s)
    -> execCode (compile e) st == denExp e \scons st
\end{code}

\noindent We will proceed by induction on the expression \ident{e}. The literal
case is trivial and solvable with \ident{refl}.

\begin{code}
  correctness (Lit x) _ = refl
\end{code}

\noindent The binary-operator case is a bit more involved and we will prove it
using equational reasoning, again.

\begin{code}
  correctness (Binop op l r) st = begin
    execCode (compile (Binop op l r)) st
      ==< refl \>
    execCode (compile r \app compile l \app [[ opInstr op ]]) st
      ==< compile\-distr (compile r) _ _ \>
    execCode (compile l \app [[ opInstr op ]]) (execCode (compile r) st)
      ==< compile\-distr (compile l) _ _ \>
    execCode [[ opInstr op ]] (execCode (compile l) (execCode (compile r) st))
      ==< cong (\lam z -> execCode [[ opInstr op ]] (execCode (compile l) z) (correctness r st) \>
    execCode [[ opInstr op ]] (execCode (compile l) (denExp r \scons st))
      ==< cong (\lam z -> execCode [[ opInstr op ]] z) (correctness l st) \>
    execCode [[ opInstr op ]] (denExp l \scons denExp r \scons st)
      ==< refl \>
    execInstr (opInstr op) (denExp l \scons denExp r \scons st)
      ==< op\-correct op \>
    denOp op (denExp l) (denExp r) \scons st
      ==< refl \>
    denExp (Binop op l r) \scons st
    \qed
\end{code}

\noindent The first \ident{refl} is used to expand the definition of compile
for the \ident{Binop} case so that human readers can see what's going on more
easily.

Then we make two appeals to the lemma \ident{compile-distr}. Each usage of this
lemma removes a part of the code sequence (exactly corresponding to an operand
of the binary operator being compiled) and transforms it to the effect that this
piece of code has on the stack until only a single instruction is left in the
code sequence.\footnote{Note that we omit two of three arguments of
\ident{compile-distr} in both applications. This omission improves readability
of the proof and Agda can infer these terms, anyway.}

The following two rather cryptic steps use the function \ident{cong} that allows
us to prove equality of two terms, given a proof of equality of their subterms
in a common context.

\begin{code}
  cong : forall {a b : Set} {x y : a}
    -> (f : a -> b)
    -> x == y -> f x == f y
\end{code}

\noindent This function is used with recursive applications of the theorem
\ident{correctness} to both operands of the binary operator. This allows us
to rewrite the subterms in the form \ident{execCode (compile operand) state}
to the form \ident{denExp operand \scons state}, which is equivalent. These
two recursive applications are actually inductive hypotheses.

\note{In a way, the two steps using \ident{exec-distr} and \ident{correctness}
for each operand actually correspond to ,,accelerated execution'' of these
pieces of code -- we do not execute the instructions; instead, we rely on the
induction hypothesis to simultaneously remove the code corresponding to the
operand and push its denotation onto the stack.}

Finally, we use the lemma \ident{op-correct} to show that executing the
leftover instruction is exactly what is left to do to get the desired value
on top of the stack.

\subsection{Remarks}

Totality of Agda functions gives us a proof of termination of this algorithm and
the above correctness proof gives us a guarantee that the compiler calculates
the correct code, given the defined semantics.

This is a very strong guarantee and it did not cost us that much -- the code we
have written looks much like the equivalent in any other functional language.
However, we have been maintaining much stronger invariants along the way, being
able to, for example, afford including only \emph{relevant}\footnote{In this
  context, by \emph{relevant} we mean the cases that arise during normal and
  expected operation of the program; for example, as already mentioned, we
needn't specify what to do when the instruction \ident{ADD} gets an
inappropriate number or types of elements on the stack -- just because this
cannot happen \emph{and the compiler knows it}.} pattern cases in a completely
safe way, without triggering compiler warnings.

Implementation-wise, the above sections form separate modules in the
accompanying Agda code and these modules define the overall structure of our
development. In the following chapter, we will develop
the code further by extending and improving particular modules.

