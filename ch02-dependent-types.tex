\chapter{Dependently typed programming in Agda}
\label{chap:dependent-types}

We will create our development in a dependently typed pure functional language named Agda.
This chapter provides a (very) brief introduction to Agda, along with some commentary about
related topics.

%\todo{...and also presents reasons why Agda was chosen as the implementation language. Does it present the reasons?}

The aim is to provide the bare necessities for reading this thesis if the reader is not
familiar with Agda or dependently typed programming -- it is mostly an overview of Agda syntax
and some basic techniques used.

%\todo{Not only language but also logic. Per Martin-L\"{o}f's Type Theory? Dependent types. 
%The Curry-Howard correspondence. Total functional programming.
%Structural recursion and termination. Formal verification. The Agda proof assistant.}

For a much more complete introduction to Agda and dependently typed programming, see the
tutorial by Norell \cite{norell08} or any other tutorial from the Agda wiki \cite{agda-wiki}.

\section{A simple exceptionless language}
\label{sec:simple-language}

As a brief Agda crash-course, we will implement a very simple compiler for
a~language of arithmetical expressions to instructions for a stack machine.

Being a simple and instructive task, this has been done many times in a variety
of programming languages in other papers, publications, blog posts, et cetera;
for example in Epigram \cite{epigram-compiler} or in Coq \cite{chlipala:compiler}.

All high-level languages in this thesis will be languages of simple, typed
expressions and our first language will feature only natural numbers, addition,
and no exceptions. Besides demonstrating how dependent programming in Agda
looks like, we will also implement the auxiliary ecosystem of the
compiler and the skeleton of our development, upon which we will build later
throughout the thesis.

\todo{Add a reference to the source code.}

% branch: no-exceptions

\subsection{Type universe}

It is probably reasonable to expect a programming language to be able to represent
expressions of different types (e.g. Integer or Boolean).

Hence the first thing we will introduce is the type universe representing the set of
types of expressions of our high-level language. We will \emph{index} our types with
values of this type, most notably the type \ident{Exp} of expressions, to
indicate what the type of the expression is.

While we could use Agda
types directly for indexing, with an explicit universe, we get decidable
equality of expression types\footnote{We do not use this property but we would
if we had to implement type-driven handler selection.} and all datatypes conveniently
in \ident{Set}.%
\footnote{To avoid size issues, appearing in Agda in the form of Girard's Paradox
\cite{girard:dissertation} (or a simplification thereof by Hurkens \cite{hurkens}),
Agda features stratified type universes with universe polymorphism.

Our approach, modeling the types of our expression language as values
of the universe \ident{U} together with an interpretation function,
lets us have all types flat in the lowest Agda type
universe, called \ident{Set}, relieving us from the burden of caring about the
stratification.}

\note{In this thesis, we will be a bit lax on wording related to type
universes.  We will be talking about ``expressions of the type \ident{u}'',
while actually referring to ``terms that represent expressions of \emph{the
type denoted by \ident{u}}''. This simplified approach can hardly cause
confusion, while adhering to precise wording in every situation at all costs
would sacrifice comprehensibility.}

\subsubsection{Data type declarations}

Here we arrive at Agda \emph{type declarations}. These resemble GADTs from
Haskell or type declarations from other dependently typed languages, like Coq.
The first line contains the name of the data type and the Agda universe we want to
put the type in. The following lines contain data constructor declarations,
including an explicit type for each data constructor.

As a general rule, Agda uses indentation instead of punctuation to delimit
blocks. Hence the constructors \emph{must} be indented; in return we get
visually clean code.

\begin{code}
  data U : Set where
    nat : U
\end{code}

\noindent Hence, the above declaration introduces a single constant named \ident{nat},
that has the type \ident{U}. This universe could not be much simpler. While we aim to support more
types at a later stage, now we restrict the language to express only natural
numbers, for the sake of simplicity. As long as we build the program to
distinguish different types, this is just a quantitative difference.

Being so simple, this is hardly a representative example of a data type
declaration. A bit more elaborate example follows shortly in \Fref{sec:simple-exp}.

\subsubsection{Interpretation function}

We will also need an interpretation function that maps
types of our simple language to Agda types so that we can use Agda values in
the modeled language, talk about its denotational semantics, etc.

Function declarations in Agda consist of a type declaration and a
number of clauses describing the outcomes of the function depending on
its arguments.

\begin{code}
  el : U -> Set
  el nat = Nat
\end{code}

\noindent There is one clause/equation per distinguished combination of arguments. For
example the universe \ident{U} used in a modified version of this function in \Fref{sec:lin-el}
contains another value \ident{bool : U}. The function \ident{el} then contains another clause
in the form:
\begin{code}
  el bool = Bool
\end{code}
Users of languages belonging (syntactically) to the Miranda family, for example Haskell, Clean
or Idris, or even the SML branch of the ML family, will feel safely at
home here. The difference here to the other ML-style languages, like OCaml, F\# or Coq, is that
instead of a big match
expression on the right side of a single equation, there are multiple equations where pattern
matching takes place on the left side of each equation.\footnote{Seen for example in
the definition of the function \ident{denExp} in \Fref{sec:simple-denExp}.}

Also note that in Agda, it is possible for a function to return a \emph{type}, in contrast
with languages like Haskell or OCaml. This is possible because in dependently typed languages,
types, kinds, sorts etc. are all first-class values.

Furthermore, beyond standard type checking, Agda, being a \emph{total} functional
programming language, performs two special checks on every function definition.
\begin{itemize}
	\item The \emph{coverage checker} checks that pattern coverage of every
		function definition is exhaustive. In other words, whatever arguments
		are given to the function, \emph{some} pattern from its definition must match
		them.
	\item The \emph{termination checker} checks that recursion used (if any) by the
		given function is structural and hence this function is obviously terminating.
\end{itemize}
This ensures that every function terminates on every input, which is important for soundness
of proofs. If functions were allowed to not terminate, the following circular argument would
typecheck as a proof of the apparently false proposition:
\begin{code}
  falsum : forall {a : Set} -> a
  falsum = falsum
\end{code}
Put another way, if a function never returns, it can ``promise'' to return anything, making
its type signature meaningless.

\subsection{Expressions}

The core of the high-level language we are going to model consists of its expressions, of
course. For now, we will support nothing more than (numeric) literals and addition.
However, for further extensibility, we separate the type of binary operators.

\subsubsection{Operators}

The type family of binary operators is indexed by the types of the two values that
the operator accepts as arguments; the third index represents the type of
the result of application of the operator on the two values.

\begin{code}
  data Op : U -> U -> U -> Set where
    Plus : Op nat nat nat
\end{code}

\noindent This means that a value of the type \ident{Op u v w} represents
a binary operator whose operands have the types \ident{u} and \ident{w},
and whose result has the type \ident{w}. Hence, the value \ident{Plus}
represents an operator that takes two \ident{nat}s and returns a \ident{nat}.%
\footnote{We will add more operators later. How the constructors representing
different operators are typed can be seen in \Fref{sec:more-operators}.}

\subsubsection{Inductive type families}

To put the above type declaration in context from the theoretical point of view,
we should briefly discuss various ways how types may be declared in Agda.
One of the most elementary kind of a type declaration follows.
\begin{code}
  data Color : Set where
    Red : Color
    Orange : Color
    Green : Color
\end{code}
This declaration declares a finite type inhabited by three values represented by 
the three data constructors \ident{Red}, \ident{Orange}, and \ident{Green}. Such declarations
are usually possible even in non-functional languages, often called \emph{enums}.

In Agda however, data constructors may take arguments, as shown in the following snippet.
\begin{code}
  data Observation : Set where
    LightsOn : Bool -> Observation
    PersonsPresent : \bN -> Observation
\end{code}
Hence we get a type containing values like \ident{LightsOn false} or \ident{PersonsPresent 3},
representing some observations a program might need to model.

What's more, data types can be \emph{recursive}. Instead of the simplest recursive type,
the type of natural numbers, let us declare the type of lists of natural numbers.
\begin{code}
  data NList : Set where
    \NIL : NList
    _::_ : \bN -> NList -> NList
\end{code}
The above declaration says that a list of natural numbers is either the empty list \ident{[\,]}
or a \emph{cons} (i.e. \ident{\_::\_}) of a natural number and a list of natural numbers.

In Agda, the term \emph{inductive types} is used instead of the term \emph{recursive types}
for the above kind of declarations\footnote{Agda also provides means to declare and use
\emph{coinductive} data types.}. This refers to the totality of the language
and finiteness of such data structures, which allows for simple inductive reasoning
\cite[Sec.~3.1, Remark]{bove-dybjer}.

In practice, the declaration of \ident{NList} is too restrictive because it allows to store
only natural numbers in lists. We can lift this restriction by adding a \emph{parameter} to
the data type.
\begin{code}
  data List (A : Set) : Set where
    \NIL : List A
    _::_ : A -> List A -> List A
\end{code}
The first line of the declaration says that if the type constructor \ident{List} is applied
to some parameter \ident{A} being a type, the resulting expression \ident{List A} is itself
a type. This resulting type represents the lists whose elements have the type \ident{A},
so our original type of natural-number lists is written as \ident{List $\bN$}.

Such types are called \emph{parameterized types} because they abstract out parts of the
declaration as parameters that may be chosen arbitrarily\footnote{Arbitrarily within the given
type. In our case, \ident{A : Set} so the parameter \ident{A} represents any type.}.
As already mentioned, in the case
of lists, the single parameter \ident{A} denotes the type of the elements contained in a list.

Now suppose we want to write a function \ident{head : $\forall$ \{A\} $\to$ List A $\to$ A}.
We cannot do that, because the list might be empty and there is nothing to return in that case,
and Agda does not allow partial functions. We should probably make the function accept only
non-empty lists\footnote{The other option would be to return \ident{Maybe A} but sometimes
it is not appropriate.}.

For that purpose, we need to extend our parameterized type of lists further into
an~\emph{indexed type family} by adding another argument to the type constructor \ident{List}%
\footnote{Traditionally, length-indexed lists are called \emph{vectors} so we will also
change the name of the constructor to \ident{Vec}.}.
The newly added argument will denote the length of the list.
\begin{code}
  data Vec (A : Set) : Nat -> Set where
    \NIL : Vec A zero
    _::_ : forall {n} -> A -> Vec A n -> Vec A (suc n)
\end{code}
The above code says that the constructor \ident{[\,]} constructs a list of any given type
with the zero length; the constructor \ident{\_::\_}, for any (implicit) length \ident{n},
takes an element of the type \ident{A} and an appropriately typed list of the length \ident{n},
yielding a list whose length is \ident{suc n}.

There are several important points where the newly added \emph{index} differs to
the \emph{parameter} \ident{A}:
\begin{itemize}

	\item Parameters appear to the left of the semicolon in the type declaration head
		and they are named because their names stay in scope throughout the whole declaration.
		Indices appear to the right of the semicolon and they are usually not named.
		Even if they are, these names are not accessible in the body of the type declaration.

	\item The parameter \ident{A} is bound in the declaration head and it is used
		uniformly within the body of the declaration, whereas the index \ident{n}
		in the constructor \ident{\_::\_} is defined locally, only for that particular
		constructor.
		
	\item In the above example, all data constructors must result in the type \ident{Vec A n}
		for some value of \ident{n}. The way the declaration is written, it is not possible to have
		a~constructor construct a~value of the type \ident{Vec (Maybe A) n}.
		
	\item Recursive occurrences of a data type must also use the parameters as they were bound
		in the declaration head.
		If \ident{A} is a parameter, the following data constructor declaration is not valid%
		\footnote{In Agda, this is a bit complicated. Type indices are \emph{usually} written
			to the right of the semicolon in the declaration head. However, Agda does allow
			using different \emph{parameters} in recursive occurrences of a data type in its
			constructors. Such ``parameters'' would be classified as indices by Dybjer
			\cite{dybjer97}, despite being to the left of the semicolon in Agda \cite{mcbride08}.}:
		\ident{\_::\_ : A $\to$ List (Maybe A) $\to$ List A}. Indices are not bound in the
		declaration head so this restriction wouldn't even make sense for them.
		
	\item Therefore, the values of parameters are ``chosen from the outside'' arbitrarily
		when the type is used and they parameterize the whole recursive structure uniformly.
		On the other hand, the values of indices are ``chosen by the constructors'': they
		may change in recursive occurrences and different data constructor may construct values
		of different types, differing in type indices.
		
		For example, any nonempty length-indexed list
		contains another list indexed with a different number, smaller by one, but they
		both store elements of the same type given by the parameter.
		
	\item Since type indices depend on the particular constructor used, pattern matching
		on different constructors yields information about the indices: if we know
		what constructor was used to construct a value, we also know how the corresponding
		type indices are related to other values.
		
		Conversely, this correspondence can
		also rule out some constructors: if we have a list whose type is indexed by \ident{zero},
		we can be sure that the list was not constructed by the constructor \ident{\_::\_}
		since \ident{zero} is not equal to \ident{suc n} for any \ident{n}; these expressions
		do not unify.
		
\end{itemize}
In non-dependent functional languages like Haskell or OCaml, (type-) parameterized
types are generally available\footnote{Although these too allow definitions like
\ident{data AltList a b = Nil | Cons a (AltList b a)}.}. (Type-) indexed type
families are available for example in Haskell in different forms; the form closest to
the inductive families of Agda are GADTs \cite[p.~178]{haskell-manual}.

Now that we have declared the length-indexed type family of vectors, we are ready to write
the function \ident{head} that is completely safe and total.
\begin{code}
  head : forall {A n} -> Vec A (suc n) -> A
  head (x :: _) = x
\end{code}

First, note that the type signature says that the function accepts only nonempty vectors;
the length index \ident{zero} of the empty vector would not unify with the required form
\ident{suc n}.

Second, note that we used only one clause in the definition and the clause for the empty-list
case is missing. Agda sees that the type of the argument rules out the constructor \ident{[\,]}
and does not require us to write a clause for that case.

This concludes our discussion of parameterized types and indexed type families. We will use
the above principles in the code that follows.

\begin{comment}
Dependently typed languages with inductive types, like Agda, generally support inductive
type families. An inductive type family is a collection of inductive types given by
a common (generally $n$-ary) type constructor. Specific types from this family are then obtained
by saturating the type constructor with arguments of the appropriate types.

For example, the above type family given by the type constructor \ident{Op} is indexed
by three values of the type \ident{U}. By providing such specific values \ident{u}, \ident{v},
and \ident{w}, we instantiate the concrete type \ident{Op u v w} belonging to this type family.

Superficially, type families might resemble parameterized types, as found in other functional
languages such as Haskell or OCaml: in both cases, types are constructed by saturating a type
constructor.

\todo{Wouter Swierstra July 31, 2012 10:52 AM: This paragraph is a little vague. Inductive type families are very different from parameterized types (such as lists or trees). This isn't clear from the text as it stands. Perhaps explain a bit about how pattern matching works, or the difference between GADTs and regular data types. I'm sure some of the Agda tutorials have some text you can use to word this a bit more precisely. \cite[Sec.~2.6 and 3.1]{bove-dybjer}}

Some non-dependently typed languages, like Haskell, also provide a limited form of
type families, indexed only by types, not arbitrary values. For example, in Haskell,
``\ident{Expression Int}'' may represent a valid type (and it often does) but
``\ident{VectorOfLength 3}'' may not.%
\footnote{In Haskell, suitable type declarations allow expressions like \ident{S (S (S Z))}
to appear in type signatures. However, these expressions are proper \emph{types}, not values.}
\end{comment}

\subsubsection{Expressions}

Now we can define the expressions: literals and binary operators.
Note that also this type family is indexed with elements of the type universe
\ident{U} so that the Agda type of an expression also incorporates the type of
the expression in the modelled language.

\begin{code}
  data Exp : U -> Set where
    -- Literals
    Lit : forall {u} -> el u -> Exp u
    -- Binary operators
    Bin : forall {u v w} -> Bin u v w -> Exp u -> Exp v -> Exp w
\end{code}\label{sec:simple-exp}

\noindent This is a bit more elaborate declaration using several Agda features.
\begin{itemize}
	\item Agda supports \emph{Unicode} in its source files and is very liberal about what
		characters may occur in tokens. This enables Agda source code to get much closer
		to the standard math notation. Hence the $\forall$ and $\to$ characters
		are contained in it literally.
		
		What's more, Agda does not classify tokens as operators or identifiers
		on a~lexical basis; (almost) anything may be an infix or mixfix operator,
		if properly declared\footnote{Discussed in \Fref{sec:fixity}.} and ordinary
		identifiers may consist purely of special characters.
		This also means that operators must be surrounded by whitespace: for example,
		``\ident{foo-bar}'' parses as a single identifier containing a hyphen.
		
	\item The names enclosed in braces are \emph{implicit arguments}, as usual
		in dependently typed languages. For example, a fully saturated application of the
		constructor \ident{Lit} contains only one argument; the implicit argument
		\ident{u} is inferred from the context of the application.
		
	\item The $\forall$ sign in front of the implicit arguments enables us to \emph{leave
		out the types} of the arguments -- these will be inferred from the context of
		the constructor declaration. An equivalent declaration of the constructor \ident{Lit}
		would be \ident{Lit : \{u : U\}  $\to$ el u $\to$ Exp u}.
		This also works with explicit (non-braced) arguments.

	\item The implicit arguments above are \emph{named}. Explicit arguments can be named, too;
		we could well write \ident{Lit : $\forall$ \{u\} $\to$ (x : el u) $\to$ Exp u}.
		However, we do not use the name \ident{x} anywhere, unlike the name \ident{u},
		so we don't need to even create it.
		
	\item Note that we \emph{apply a function in the declaration} of the constructor
		\ident{Lit}. In a language with dependent types, we can use function application
		in type declarations freely.
\end{itemize}

Given the above definition of the type family of expressions, it can be immediately seen that
literals of our expression language always carry appropriately typed values of the
type \ident{el u}.

The typing machinery also ensures that binary operators receive operands of correct
types, yielding an expression typed exactly as given by the operator type specification.

\subsection{Semantics of expressions}

Our definition of the high-level language would not be complete without giving
the denotational semantics of its expressions. This is done by the following
pair of simple functions.

\subsubsection{Operator semantics}

Semantics of operators is given by the function \ident{denOp} as follows.

\begin{code}
  denOp : forall {u v w} -> Op u v w -> el u -> el v -> el w
  denOp Plus = _+\_
\end{code}

\noindent The function \ident{denOp} can be interpreted as a function that takes a value
representing a binary operator of the type \ident{Op u v w} and returns an appropriately-typed Agda
function of the type \ident{(el u $\to$ el v $\to$ el w)}.

The function returned for the operator \ident{Plus} is the ordinary addition function
from the standard library. Surrounded with underscores, the infix operator \ident{+} becomes
a standard function identifier.\footnote{See \Fref{sec:fixity} for more information on
infix operators in Agda.}

\subsubsection{Expression semantics}

Expressions are then turned into Agda values as follows. Literals are evaluated trivially,
binary-operator expressions are evaluated using the denotation of the corresponding
operator and recursively obtained denotations of the operands.

\begin{code}
  denExp : forall {u} -> Exp u -> el u
  denExp (Lit x) = x
  denExp (Bin op l r) = denOp op (denExp l) (denExp r)
\end{code}\label{sec:simple-denExp}

\noindent As already mentioned, pattern matching happens on the left side of defining equations;
it is \emph{exhaustive}: both of the two possible constructors are covered; and recursion
is structural: both recursive applications are made to a subterm of the argument of the function.
Hence this function is total.

\subsection{Virtual machine}

We will use a very simple machine to run the compiled code, featuring only a stack
of values.

\subsubsection{Stack}

The stack of the machine is just a cons-list of values, indexed by the list
of types (elements of the universe \ident{U}) of the values pushed on the stack.
This means that just by looking at the type of the stack, we can tell how many
elements it contains and what types they have. Let us first define the type
of stack shapes.

\noindent \begin{minipage}{\textwidth}
\begin{code}
  -- open import Data.List
  infixr 5 _::_
  data List (a : Set) : Set where
    \NIL : List a
    _::_ : a -> List a -> List a
\end{code}
\end{minipage}

\begin{code}
  Shape : Set
  Shape = List U
\end{code}
\label{sec:fixity} In the definition of the standard list data type above,
we encountered a declaration of an infix cons constructor. While it is true that
an infix operator surrounded by underscores becomes an ordinary identifier,
Agda goes much further and permits (almost) arbitrary prefix, infix, postfix
and mixfix operators with an arbitrary number of underscores.

Declaring an identifier containing underscores modifies the Agda parser
to recognize occurrences of that identifier where the underscores have been
replaced by subexpressions. Combined with Unicode support and the (practical) absence
of lexical rules, this is a very powerful device. A few examples (some coming
from the standard library) can be seen
in \Fref{tab:mixfix}.

\begin{table}[htp]
\centering
\begin{tabular}{lll} \toprule
\textit{Mixfix form} & \textit{Applicative form} & \textit{Description} \\ \midrule
\ident{x :: xs}		& \ident{\_::\_ x xs} 			& standard infix \\
\ident{x !}			& \ident{\_! x}					& postfix factorial \\
\ident{$-$[ x +1]}	& \ident{$-$[\_+1] x}	& negative whole number constructor \\
\ident{$\langle$ 2 * x $\rangle$} & \ident{$\langle\_\rangle$ (2 * x)}& non-trivial constructs work fine \\
\ident{x + 1 * y}	& \ident{\_+\_ x (\_*\_ 1 y)}	& fixity and precedence work as defined \\
\ident{if x then y else z} & \ident{if\_then\_else\_ x y z} & mixfix with non-symbols \\
\ident{x $\lhd$ $\varepsilon$} & \ident{$\_\!\lhd\!\_$ x $\varepsilon$} & unicode \\
\bottomrule
\end{tabular}
\caption{Agda mixfix operator examples}
\label{tab:mixfix}
\end{table}

The declaration \ident{\infixr\ 5 \_::\_} gives fixity and precedence for the operator.
The higher the number, the tighter the operator binds. Associativity is then given
by the variant of the declaration: \ident{\infixr} declares a right-associative operator,
\ident{\infixl} declares a left-associative operator, \ident{\infix} declares a non-associative
operator.

Let us return to the compiler. Having defined the type of stack shapes, we can
proceed to a definition of the type of stacks.

\begin{code}
  infixr 5 _\scons\_
  data Stack : Shape -> Set where
    snil : Stack \NIL
    _\scons\_ : forall {u s} -> el u -> Stack s -> Stack (u :: s)
\end{code}
The literal \ident{snil} represents the empty stack; new values are
pushed onto it using the infix constructor \ident{\bin{\scons}}.

Note that pushing a value on the stack changes the type of the stack: the shape
index gets prefixed by the type of the value pushed. Given that the empty stack
is indexed by the empty shape, we always know how many items there are on the
stack and what types they have, as already mentioned above.

\subsubsection{Instructions}

At this stage, the machine supports only two instructions: \ident{PUSH}
and \ident{ADD}. This gives rise to the following type family of instructions.
\begin{code}
  data Instr : Shape -> Shape -> Set where
    PUSH : forall {u s} -> el u -> Instr s (u :: s)
    ADD : forall {s} -> Instr (nat :: nat :: s) (nat :: s)
\end{code}
The type family of instructions is indexed by their action on the stack. The first shape
argument is the required stack shape so that the instruction can be executed;
the second shape argument is the resulting shape of the stack after the
instruction has been executed.

For example, the instruction \ident{PUSH} takes any value of the type \ident{el u}
and pushes it onto a stack having any shape \ident{s}, creating a new
stack of the shape \ident{u :: s}.

The instruction \ident{ADD} represents popping two natural numbers from the
stack of any shape with two \ident{nat}s on top of it, hence
\ident{nat :: nat :: s},
and subsequently pushing their sum onto it, resulting in the shape
\ident{nat :: s}.

\subsubsection{Code}

Finally, code for the stack machine is a sequence of instructions where
type indices of subsequent instructions match. For example, if one instruction
in the sequence produces a stack of the shape \ident{nat :: nat :: s},
we want the next instruction in the code sequence to accept this shape.

If we regard
\ident{Instr : Shape $\to$ Shape $\to$ Set}
as a binary relation on \ident{Shape}, then code is the \emph{transitive reflexive closure}
of \ident{Instr}, which is already included in the Agda standard library as the
module \ident{Data.Star}.

\begin{code}
  -- require import Data.Star
  infixr 5 _<|\_
  data Star {a b : Set} (R : a -> b -> Set) : a -> b -> Set where
    \nil : forall {x} -> Star R x x
    _<|\_ : forall {x y z} -> R x y -> Star R y z -> Star R x z
\end{code}

\begin{code}
  Code : Shape -> Shape -> Set
  Code = Star Instr
\end{code}

\noindent The type of instruction sequences is indexed in exactly the same
manner as the type of separate instructions: the first index represents the
acceptable shape of stack before execution of the piece of code; the second
index represents the shape of stack after its execution.

Let us conclude this section with an utility function for concatenation of
instruction sequences, which is actually also included in \ident{Data.Star}.

\begin{code}
  infixr 5 _\app\_
  _\app\_ : forall {R x y z} -> Star R x y -> Star R y z -> Star R x z
  \nil \app ys = ys
  (x <| xs) \app ys = x <| xs \app ys
\end{code}

\subsection{Execution}

Now we will describe how the machine executes instructions, that is,
the operational semantics of the low-level language.

At this stage, the state of the machine is fully described by just its stack. This
means that there are no other state variables, registers or any additional
memory.

\subsubsection{Instructions}

First, we describe the effects of single instructions on the state of the machine,
that is, on the stack, separately.

\begin{code}
  execInstr : forall {s t} -> Instr s t -> Stack s -> Stack t
  execInstr (PUSH x) st = x \scons st
  execInstr ADD (x \scons y \scons st) = (x + y) \scons st
\end{code}

\noindent The above function simply says that
\begin{itemize}
  \item the effect of the instruction \ident{PUSH} is pushing the attached
    value onto the stack. This consistently extends the information contained
    in the type of \ident{PUSH x}.\footnote{The type is \ident{Instr s (u $::$ s)}
    -- for some \ident{u} and \ident{s} and is interpreted as ``\ident{PUSH x}
    pushes some value of type \ident{u} onto the stack''.}
    What the type does not say (and \ident{execInstr}
    does) is what this value exactly is.
  \item the effect of the instruction \ident{ADD} is popping two \ident{nat}s from
    the top of the stack and pushing their sum back.
\end{itemize}

Note that in this definition of the execution function, we already reap some
benefits of dependently typed programming.

First, of course, Agda checks types of the terms behind
the scenes and the machinery of types we have designed so far ensures that
in the case for \ident{PUSH x}, pushing the value \ident{x} always yields
a stack of the desired shape.

Second, in the case for \ident{ADD}, the types ensure that there are always two
\ident{nat}s on top of the stack and we can safely pattern-match with the
pattern \ident{x} \scons \ident{y} \scons \ident{st} -- because this match
will always succeed and no other patterns for the \ident{ADD} case are needed. 

Thus the above definition complies to the type signatures involved, which is a~relatively
solid hint of correctness, and it is \emph{total}, especially no pattern match failures
can occur. Despite this, compilers of non-dependently typed languages, like OCaml or Haskell,
would complain about non-exhaustive patterns here --- there is no way to tell them
that, for example, we needn't deal with empty stacks when executing \ident{ADD}.

%\todo{There's a paper saying that non-exhaustive matches are inevitable; insert
%a reference.}

\subsubsection{Code}

Execution of code is then just a left fold over the sequence of instructions,
accepting the initial and yielding the resulting state of the machine.

\begin{code}
  execCode : forall {s t} -> Code s t -> Stack s -> Stack t
  execCode \nil st = st
  execCode (i <| is) st = execCode is (execInstr i st)
\end{code}

\noindent Execution of empty code has no effect on the stack; if the code
contains instructions, then the first instruction is executed and on the
resulting stack, the rest of code is executed.

Note that the type signature of \ident{execCode} ensures that the code being
run modifies (the shape of) the stack consistently with its type indices.

\subsection{Compiler}

Compiling our simple high-level language for a stack machine is easy. The
central idea is that execution of an expression of some type is equivalent to
pushing its value onto the stack. Literal values are then pushed on the stack
directly; binary-operator expressions first evaluate both operands, effectively
putting their values on the top of the stack, and then execute the appropriate
instruction, determined by the operator. This instruction pops the top
two values from the stack as its operands and pushes the result back.

\label{sec:simple-compiler}\begin{code}
  -- Syntactic sugar, promote an &\cident{Instr}& to singleton &\cident{Code}&
  [[_\;]] : forall {s t} -> Instr s t -> Code s t
  [[i\;]] = i <| \nil

  -- Determine what instruction performs the required calculation
  opInstr : forall {u v w} -> Op u v w -> forall {s} -> Instr (u :: v :: s) (w :: s)
  opInstr Plus = ADD

  -- Turn the expression into code
  compile : forall {u} -> Exp u -> forall {s} -> Code s (u :: s)
  compile (Lit x) = [[ PUSH x ]]
  compile (Bin op l r) = compile r \app compile l \app [[ opInstr op ]]
\end{code}

\noindent Again, behind the scenes, Agda ensures that all types match and the
code compiled by this function will not make the stack machine fail.\footnote{
  To be fair, this is already a property of \ident{Code}, ``inherited'' by the
  function \ident{compile} via its return type.  However, it does constrain
possible definitions of the function \ident{compile}.} For example, there is no
way to have the function \ident{compile} output code where \ident{ADD} would
not get two \ident{nat}s on the top of the stack.

\subsection{Correctness}

This is the only place in this thesis where we include the full proof
of correctness. All proofs are of course contained in the attached Agda source
code.

\subsubsection{Agda as a proof assistant}

Apart from being a total dependently typed functional \emph{programming} language,
the ``total dependently typed'' part also makes Agda suitable for proving theorems.
This means that type signatures can express theorems, values of these
types correspond to their proofs, and type checking coincides with proof checking,
as already mentioned in \Fref{sec:intro-certified-programming}.

Agda ships with an interactive Emacs editor mode, which extends Agda to a~proof
assistant and greatly helps writing both proofs and Agda code in general. A~(rather brief) guide
to the Emacs \ident{agda-mode} can be found on the Agda wiki \cite{agda-mode}.

\subsubsection{Propositional equality}

In the following proofs, we will use the operator $\equiv$ to denote
\emph{propositional equality}. This is realized in Agda through the following
type family.

\begin{code}
  data _==_ {a : Set} (x : a) : a -> Set where
    refl : x == x
\end{code}

\noindent The definition implies that all nonempty members (inhabited by \ident{refl}) of this
family must have the index the same as the parameter.
Therefore conversely, if we have a~value \ident{refl : a $\equiv$ b}, it must be the case that
\ident{a} is the same\footnote{The proper term is \emph{definitionally equal}. Agda has an
internal notion of definitional equality, based on comparing normal forms.} as \ident{b}.

This is taken into account by Agda when doing pattern matching on the constructor
\ident{refl} and causes unification of the corresponding type variables. Such behavior is not special
to propositional equality at all -- after all, propositional equality is expressed by
an ordinary type family -- it is just a consequence of a more general unification mechanism,
which works this way for any other data type.

\subsubsection{Operator lemma}

There are two auxiliary lemmas that we will need to prove our main result. The
first one of them is called \ident{op-correct} and it says that for any binary
operator, the instruction picked by the compiler indeed does what the
denotation of the binary operator says.

To be more specific, for any operator \ident{op} and two values \ident{x} and
\ident{y} of appropriate types, executing \ident{opInstr op} with the two
values on top of the stack results in having the value \ident{denOp op x y}
on the top of the stack afterwards.

\label{sec:cor-op-correct}\begin{code}
  op\-correct : forall {s u v w} {st : Stack s} {x : el u} {y : el w}
    -> (op : Op u v w)
    -> execInstr (opInstr op) (x \scons y \scons st) == denOp op x y \scons st
  op\-correct Plus = refl
\end{code}

\noindent In the case for \ident{Plus}, Agda substitutes the term \ident{Plus}
for the variable \ident{op} in the appropriate places in the equality, normalizes
it (expanding function definitions etc.) and the proof becomes a trivial observation of
identity of normal forms, which is indicated by \ident{refl}.

\subsubsection{Distributivity lemma}

The other lemma we will need says that execution of code distributes over
concatenation of code. In other words, executing the code \ident{c $\lhd\!\lhd$
d} has the same effect as first executing \ident{c} and then executing
\ident{d} on the resulting stack.

\label{sec:cor-compile-distr}\begin{code}
  compile\-distr : forall {s t u} {st : Stack s}
    -> (c : Code s t) -> (d : Code t u)
    -> execCode (c \app d) st == execCode d (execCode c st)
\end{code}

\noindent We will proceed by induction on the parameter \ident{c}, which yields
two cases: either \ident{c} is empty or it consists of an instruction and the
rest of code. The first case is trivial by substituting $\varepsilon$ for the variable
\ident{c} in the equality and observing identity of the normal forms on both sides.

\begin{code}
  compile\-distr \nil d = refl
\end{code}

\noindent For writing the proof for the second case, we will use the wonderful
way supported by the Agda module \ident{$\equiv$-Reasoning}\footnote{Actually,
there are also other similar modules, like \ident{$\le$-Reasoning} etc.}, which
lets us write proofs in the equational-reasoning style; appearing just the way
it would if we did it with pen and paper.\footnote{This is achieved by clever
mixfix hackery and Unicode usage.}

\begin{code}
  compile\-distr {st} (i <| is) d = let open ==\-Reasoning in begin
    execCode (i <| is \app d) st
      ==< refl \>
    execCode (is \app d) (execInstr i st)
      ==< compile\-distr is d \>
    execCode d (execCode is (execInstr i st))
      ==< refl \>
    execCode d (execCode c st)
    \qed
\end{code}

\noindent The proof begins with the first line, which is usually exactly the
left-hand side of the equality we aim
to prove.  The second line contains the proof that the first line is equal to
the third line and so on -- by alternating terms and equality proofs, we can
gradually rewrite the left-hand term to the right-hand term of the desired
equality.

The first proof is just comparison of normal forms, as indicated by
\ident{refl}. In this step, we just unfold the definition of \ident{execCode},
immediately obtaining the next term.

The second proof uses \ident{compile-distr} recursively as the induction
hypothesis to break the execution of concatenated code into two stages: first
executing \ident{is}, then executing \ident{d}.

The third proof is just \ident{refl} again and we use it to restructure the
term to the desired final form, this time \emph{folding} the longer
subterm to its definitional equivalent \ident{execCode c st}.

\subsubsection{A shorter proof of distributivity}

Since \ident{refl} is a certificate of identity of normal forms, by rewriting
a term to a different form using \ident{refl} as the proof, the normal form of
that term remains the same. As normal forms is what Agda compares when typechecking,
we can omit the intermediate \ident{refl}-based rewrites, which leaves us
with just one proof in the chain.\footnote{Since a chain consists of equality
proofs connected with transitivity of equality, a singleton chain is identical
to the single proof itself.}

\begin{code}
  compile\-distr : forall {s t u} {st : Stack s}
    -> (c : Code s t) -> (d : Code t u)
    -> execCode (c \app d) st == execCode d (execCode c st)
  compile\-distr \nil d = refl
  compile\-distr (i <| is) d = compile\-distr is d
\end{code}

\noindent However, often human readability is more desirable than terseness of
the code and in such cases, the equational proof may be more appropriate.

\subsubsection{Main correctness theorem}

This is the central result of this stage that relates together everything we
have defined so far in a single proof of correctness.

This proof formalizes the idea that we informally mentioned when we started to
write the compiler: executing the compiled code for an expression should be
equivalent to pushing the value of the expression (as given by the denotational
semantics) onto the stack.

\label{sec:cor-correctness}\begin{code}
  correctness : forall {u s}
    -> (e : Exp u) (st : Stack s)
    -> execCode (compile e) st == denExp e \scons st
\end{code}

\noindent We will proceed by induction on the expression \ident{e}. The literal
case is trivial and solvable with \ident{refl}.

\begin{code}
  correctness (Lit x) _ = refl
\end{code}

\noindent The binary-operator case is a bit more involved and we will prove it
using equational reasoning, again.

\begin{code}
  correctness (Binop op l r) st = begin
    execCode (compile (Binop op l r)) st
      ==< refl \>
    execCode (compile r \app compile l \app [[ opInstr op ]]) st
      ==< compile\-distr (compile r) _ _ \>
    execCode (compile l \app [[ opInstr op ]]) (execCode (compile r) st)
      ==< compile\-distr (compile l) _ _ \>
    execCode [[ opInstr op ]] (execCode (compile l) (execCode (compile r) st))
      ==< cong (\lam z -> execCode [[ opInstr op ]] (execCode (compile l) z) (correctness r st) \>
    execCode [[ opInstr op ]] (execCode (compile l) (denExp r \scons st))
      ==< cong (\lam z -> execCode [[ opInstr op ]] z) (correctness l st) \>
    execCode [[ opInstr op ]] (denExp l \scons denExp r \scons st)
      ==< refl \>
    execInstr (opInstr op) (denExp l \scons denExp r \scons st)
      ==< op\-correct op \>
    denOp op (denExp l) (denExp r) \scons st
      ==< refl \>
    denExp (Binop op l r) \scons st
    \qed
\end{code}

\noindent The first \ident{refl} is used to expand the definition of compile
for the \ident{Binop} case so that human readers can see what's going on more
easily.

Then we make two appeals to the lemma \ident{compile-distr}. Each usage of this
lemma removes a part of the code sequence (exactly corresponding to an operand
of the binary operator being compiled) and transforms it to the effect that this
piece of code has on the stack until only a single instruction is left in the
code sequence.%
\footnote{Note that by using underscores instead of proper expressions,
we let Agda infer two of three arguments of \ident{compile-distr} in both applications,
which improves readability of the proof.

An underscore in an expression context means ``please infer this value'' and
if the value is uniquely determined from the context, Agda will accept the
underscore instead of the (possibly convoluted) inferrable sub-expression.}

The following two rather cryptic steps use the function \ident{cong} that allows
us to prove equality of two terms, given a proof of equality of their subterms
in a common context.

\begin{code}
  cong : forall {a b : Set} {x y : a}
    -> (f : a -> b)
    -> x == y -> f x == f y
\end{code}

\noindent This function is used with recursive applications of the theorem
\ident{correctness} to both operands of the binary operator. This allows us
to rewrite the subterms in the form \ident{execCode (compile operand) state}
to the form \ident{denExp operand \scons state}, which is equivalent. These
two recursive applications are actually inductive hypotheses.

\note{In a way, the two steps using \ident{exec-distr} and \ident{correctness}
for each operand actually correspond to ``accelerated execution'' of these
pieces of code -- we do not execute the instructions; instead, we rely on the
induction hypothesis to simultaneously remove the code corresponding to the
operand and push its denotation onto the stack.}

Finally, we use the lemma \ident{op-correct} to show that executing the
leftover instruction is exactly what is left to do to get the desired value
on top of the stack.

\subsubsection{``With'' patterns}

There is one syntactic feature of Agda left that we have not covered yet but that will
occur in the following chapter: the \emph{with-patterns} \cite[Section~2.6]{norell08}.

Suppose we wanted to write a function named ``\ident{first}'' that,
given a (decidable) predicate and a list, returns
the first element in the list satisfying the given predicate. We don't want to
do it manually; instead, we will use the library function \ident{filter} that,
given a predicate and
a list, returns the list of \emph{all} elements satisfying the given predicate.

One way to do it is to define an auxiliary function and compose it with the function
\ident{filter}:
\\ \begin{minipage}{\textwidth}
\begin{code}
  safeHead : {a : Set} -> List a -> Maybe a
  safeHead \NIL = nothing
  safeHead (x :: _) = just x
  
  first : {a : Set} -> (a -> Bool) -> List a -> Maybe a
  first p = safeHead \o filter p
\end{code}\end{minipage}\\
Sometimes however, especially with long and complicated type signatures,
defining auxiliary functions gets quite inconvenient and these functions,
unlike \ident{safeHead}, are most probably very specific for one purpose only
and not reusable. After all, a~Haskell programmer might write:
\begin{code}
  first :: (a -> Bool) -> [a] -> Maybe a
  first p xs = case filter p xs of
    \NIL -> Nothing
    y:ys -> Just y
\end{code}
Agda does not have case-expressions because in a dependent setting, pattern matching may yield
information. For example, when matching on a proof of equality of variables \ident{x} and \ident{y},
the information learned is that these variables should get unified.
The core concern is that if such pattern matches occur in case-expressions within a function,
the information learned from the case-match would have to influence pattern matching
in the arguments of the function.

To deal with this issue, Agda provides
\emph{with-patterns}, that effectively add arguments to the function being defined. The
usage should be obvious from an example.
\begin{code}
  first : {a : Set} -> (a -> Bool) -> List a -> Maybe a
  first p xs with filter p xs
  first p xs | \NIL = nothing
  first p xs | y :: _ = just y
\end{code}
A function can have multiple with-patterns, separated with vertical bars in both the
with-clause and pattern-matching clauses themselves.

Finally, notice that we had to write ``\ident{first p xs}'' three times redundantly.
This is not always the case --~sometimes the patterns are more specific in different
clauses~-- but whenever all patterns to the left of the vertical bar are the same,
they can be replaced by ellipses:
\begin{code}
  first : {a : Set} -> (a -> Bool) -> List a -> Maybe a
  first p xs with filter p xs
  \... | \NIL = nothing
  \... | y :: _ = just y
\end{code}
The ellipses are understood to stand for the patterns that precede the keyword \ident{with}
in the first clause. Of course, the right sides of the equations can use all variables bound
there.

\subsection{Remarks}

Totality of Agda functions gives us a proof of termination of this algorithm and
the above correctness proof gives us a guarantee that the compiler calculates
the correct code, given the defined semantics.

This is a very strong guarantee and it did not cost us that much -- the code we
have written looks much like the equivalent in any other functional language.
However, we have been maintaining much stronger invariants along the way, being
able to, for example, afford including only \emph{relevant}\footnote{In this
  context, by \emph{relevant} we mean the cases that arise during normal and
  expected operation of the program; for example, as already mentioned, we
needn't specify what to do when the instruction \ident{ADD} gets an
inappropriate number or types of elements on the stack -- just because this
cannot happen \emph{and the compiler knows it}.} pattern cases in a completely
safe way, without triggering compiler warnings.

Implementation-wise, the above sections form separate modules in the
accompanying Agda code and these modules define the overall structure of our
development. In the following chapter, we will develop
the code further by extending and improving particular modules.


